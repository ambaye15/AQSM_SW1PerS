{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27587ecd-1184-421a-a51a-1b735f36996d",
   "metadata": {},
   "source": [
    "# Tutorial: Data Analysis with Pre-Processed Motion Data  \n",
    "\n",
    "Austin MBaye\n",
    "\n",
    "Northeastern University\n",
    "\n",
    "## **Overview**\n",
    "In this tutorial, we will walk through the **core data analysis** process for our research using a `.pkl` file. This file contains de-identified motion data from all videos and studies, serving as the foundation for our analysis. \n",
    "\n",
    "###  **What’s in the `.pkl` File?**\n",
    "The `.pkl` file includes key motion tracking data extracted from MediaPipe’s **BlazePose model**, along with additional metadata:\n",
    "\n",
    "- **`keypoints`** – The **(x, y, visibility)** coordinates for each joint detected in the BlazePose model.\n",
    "- **`annotations`** – Frame-wise annotations for each video, indicating labeled behaviors.\n",
    "- **`fps`** – Frames per second (fps) of the video.\n",
    "- **`frame_count`** – Total number of frames in each video.\n",
    "- **`duration`** – Video duration in seconds.\n",
    "- **`name`** – Unique identifier for each entry, structured as (child-date_study).\n",
    "\n",
    "This `.pkl` file contains all necessary pre-processing data for the analysis, while still allowing room for further modifications or new ideas using the raw MediaPipe keypoint data.\n",
    "\n",
    "---\n",
    "\n",
    "## **AQSM-SW1PerS + MediaPipe Algorithm Walkthrough**\n",
    "The **Automated Quantification of Stereotypical Motor Movements via Sliding Windows and 1-Persistence Scoring (AQSM-SW1PerS)** pipeline follows these key steps:\n",
    "\n",
    "1.  **Data Colletion** - Optional walkthrough that shows how videos were created and MediaPipe pose inference is run\n",
    "2.  **Preprocessing** – Cleaning and structuring motion data for analysis.  \n",
    "3.  **Period Estimation** – Identifying periodic patterns in movement.  \n",
    "4.  **Optimizing Parameters $d$ and $\\tau$** – Finding the best delay embedding parameters for persistence analysis.  \n",
    "5.  **Computing Sliding Window Embedding** – Transforming motion data into time-delayed embeddings.  \n",
    "6.  **Computing Persistent Homology & Extracting Features** – Calculating topological persistence and selecting the 10 most persistent points as motion descriptors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e36fc6-60c0-436e-9598-7ba73d2fc9f8",
   "metadata": {},
   "source": [
    "# Data Collection (Optional Walkthrough)\n",
    "\n",
    "This part of the walkthrough shows how videos were created from publically available data from *Goodwin et. al.* and how to run MediaPipe inference on the videos. As we give all the critical information on the videos and provide all MediaPipe pose landmarks in our `.pkl` file, this section is not critical to any data analysis we do in other sections.\n",
    "\n",
    "The following functions are located in **`AQSM_SW1PerS/utils/video_tools.py`** and **`AQSM_SW1PerS/mediapipe_pose.py`**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b142710-6bc9-4426-9bfd-78207e4c1b7e",
   "metadata": {},
   "source": [
    "###  **Loading annotations**\n",
    "\n",
    "The functions below are to extract annotation information from the publically available data and are essential for writing the video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c963b21-30f9-4ac5-939a-823cc0d9b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54fe6a-d08c-4410-8cba-80148a2d99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definitions used to convert time representations and load annotations\n",
    "'''\n",
    "\n",
    "def getTime(s):\n",
    "    \"\"\"\n",
    "    Convert time from YYYY-MM-DD HH:MM:SS.mmm into seconds \n",
    "    \"\"\"\n",
    "    if isinstance(s, datetime):  \n",
    "        return s.timestamp()\n",
    "    else:\n",
    "        t = datetime.strptime(s, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        return t.timestamp()\n",
    "\n",
    "def getUnix(dt_str):\n",
    "    \"\"\"\n",
    "    Convert to Unix timestamp\n",
    "    \"\"\"\n",
    "    dt_obj = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    return dt_obj.timestamp()\n",
    "   \n",
    "def parse_datetime(datetime_str):\n",
    "    \"\"\"\n",
    "    Parse the datetime string\n",
    "    \"\"\"\n",
    "    format_str = '%Y-%m-%d-%H-%M-%S-%f'\n",
    "    return datetime.strptime(datetime_str, format_str)\n",
    "\n",
    "def loadAnnotations(filename):\n",
    "    \"\"\"\n",
    "    Load annotations into a dictionary format and extract UNIX times for each Good Data interval.\n",
    "    Each interval's annotations are stored in a distinct list.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    annotations = []\n",
    "    good_data = []\n",
    "\n",
    "    # Identify Good Data intervals\n",
    "    for m in root: #This line iterates over the top-level elements (or \"child\" elements) directly under the root element of the XML file.\n",
    "        for c in m: #This line iterates over the sub-elements (or \"children\") of each element m\n",
    "            if c.tag == \"LABEL\" and c.text == \"Good Data\": #This line checks if the tag name of the sub-element c is \"LABEL\" and if its text content (i.e., the text between <LABEL> and </LABEL> in the XML file) is \"Good Data\"\n",
    "                good_data_start_unix = getUnix(m.find(\"START_DT\").text)\n",
    "                good_data_end_unix = getUnix(m.find(\"STOP_DT\").text)\n",
    "                good_data.append((good_data_start_unix, good_data_end_unix))\n",
    "\n",
    "    # Process annotations for each Good Data interval\n",
    "    \n",
    "    for start_unix, stop_unix in good_data:\n",
    "        anno = []\n",
    "        for m in root:\n",
    "            start = -1\n",
    "            stop = -1\n",
    "            label = \"\"\n",
    "            for c in m:\n",
    "                if c.tag == \"START_DT\":\n",
    "                    start = getTime(c.text) - start_unix\n",
    "                elif c.tag == \"STOP_DT\":\n",
    "                    stop = getTime(c.text) - start_unix\n",
    "                elif c.tag == \"LABEL\":\n",
    "                    label = c.text\n",
    "\n",
    "            # Only append valid annotations within the Good Data interval\n",
    "            if 0 <= start < (stop_unix - start_unix) and stop > 0:\n",
    "                start = max(start, 0)\n",
    "                stop = min(stop, stop_unix - start_unix)\n",
    "                anno.append({\"start\": start, \"stop\": stop, \"label\": label})\n",
    "        annotations.append(anno)\n",
    "    return annotations, good_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164e0e6-0cad-4d35-8c5b-cd991bc82913",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you have access to the data folder of .xml files, then change the path below to the correct directory and person ID you wish to analyze. We will be using 001-01-18-08 for the entire tutorial\n",
    "'''\n",
    "\n",
    "from AQSM_SW1PerS.utils.paths import get_data_path\n",
    "\n",
    "data_folder = 'Study2/001-2010-05-25/Annotator1Stereotypy.annotation.xml'\n",
    "data_path = get_data_path(\"data\", data_folder)\n",
    "\n",
    "annotations, good_data = loadAnnotations(data_path)\n",
    "\n",
    "#Look at annotations\n",
    "for anno in annotations:\n",
    "    print(np.vstack(anno))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80df2a-7824-4656-b2a6-ad8fed8a62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definitions used to write videos from other data file that is publically available.  \n",
    "'''\n",
    "\n",
    "def get_current_label(annotations, current_time):\n",
    "    \"\"\"\n",
    "    Get the annotation label for the current frame based on the time within the interval.\n",
    "    \"\"\"\n",
    "    for annotation in annotations:\n",
    "        if annotation[\"start\"] <= current_time <= annotation[\"stop\"]:\n",
    "            if  annotation[\"label\"] == 'Rock' or annotation[\"label\"] == 'Flap' or annotation[\"label\"] == 'Flap-Rock':\n",
    "                return annotation[\"label\"]\n",
    "    return \"Normal\"\n",
    "\n",
    "\n",
    "def findFrames(folder_path, start_unix, end_unix):\n",
    "    '''\n",
    "    This is necessary to get the total number of frames needed to copute the fps of the video\n",
    "    '''\n",
    "    total_frames=0\n",
    "     # Use glob to get all image paths in nested folders\n",
    "    image_paths = sorted(glob.glob(f\"{folder_path}/**/*.jpg\", recursive=True))\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        img_name = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "        timestamp = parse_datetime(img_name)\n",
    "        frame_time = getTime(timestamp)\n",
    "\n",
    "        # Check if frame is within the \"Good Data\" interval\n",
    "        if start_unix <= frame_time <= end_unix:\n",
    "            total_frames += 1\n",
    "    return total_frames\n",
    "\n",
    "\n",
    "def write_video(folder_path, output_name, good_data, annotations):\n",
    "    \"\"\"\n",
    "    This takes in the files from autism data and converts them into videos, one for each 'Good Data' interval.\n",
    "    Each frame is labeled with its annotation, and the annotations for each video part are saved in a list.\n",
    "    \n",
    "    :param folder_path: Root folder containing image files in a structured directory.\n",
    "    :param output_name: Base name for output video files.\n",
    "    :param good_data: List of tuples with start and end UNIX times for each interval.\n",
    "    :param annotations: List of dictionaries with 'start', 'stop', and 'label' keys for each annotation.\n",
    "    :return: List of lists, where each sublist contains annotations for each video part.\n",
    "    \"\"\"\n",
    "    video_annotations = []  # List to store annotations for each video part\n",
    "\n",
    "    for i, (start_unix, end_unix) in enumerate(good_data):\n",
    "        part_annotations = []  # List to store annotations for the current part\n",
    "        \n",
    "        total_frames = findFrames(folder_path, start_unix, end_unix)\n",
    "        fps = total_frames/(end_unix - start_unix)\n",
    "        # Set up video writer for each part\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(f'{output_name}_part_{i+1}.mp4', fourcc, fps, (352, 288))  # Adjust dimensions as needed\n",
    "\n",
    "        # Use glob to get all image paths in nested folders\n",
    "        image_paths = sorted(glob.glob(f\"{folder_path}/**/*.jpg\", recursive=True))\n",
    "\n",
    "        for img_path in image_paths:\n",
    "            img_name = os.path.basename(img_path).split('.')[0]\n",
    "            timestamp = parse_datetime(img_name)\n",
    "            frame_time = getTime(timestamp)\n",
    "\n",
    "            # Check if frame is within the \"Good Data\" interval\n",
    "            if start_unix <= frame_time <= end_unix:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue  # Skip if image could not be read\n",
    "\n",
    "                # Get the annotation label for the current frame\n",
    "                annotation_label = get_current_label(annotations[i], frame_time - start_unix)\n",
    "                num_label = 0\n",
    "                if annotation_label:\n",
    "                    if annotation_label == 'Rock':\n",
    "                        num_label = 1\n",
    "                    elif annotation_label == 'Flap':\n",
    "                        num_label = 2\n",
    "                    elif annotation_label == 'Flap-Rock':\n",
    "                        num_label = 3  \n",
    "                        \n",
    "                    part_annotations.append(num_label)  # Store annotation for this frame\n",
    "                    # Overlay the annotation label onto the frame\n",
    "                    cv2.putText(img, annotation_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    part_annotations.append(num_label)\n",
    "                # Write the frame to the video file\n",
    "                out.write(img)\n",
    "                cv2.imshow('Frame',img)\n",
    "        # Finalize the video for this part\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Add the collected annotations for this part to the overall list\n",
    "        video_annotations.append(part_annotations)\n",
    "\n",
    "    return video_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a3726-d697-4ab4-939e-4d99ec892b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_path = 'Study1/001/URI-001-01-18-08/2008-01-18' #Replace with the path of the folder with images\n",
    "\n",
    "folder_path = get_data_path(\"autismvideos\", video_path)\n",
    "\n",
    "output_video = '001-01-18-08' \n",
    "\n",
    "frame_annotations = write_video(folder_path, output_video, good_data, annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bb78b-6885-4d93-98a3-42a537436b43",
   "metadata": {},
   "source": [
    "###  **MediaPipe & YOLOv5 Pose Estimation**: The following functions can be located in `**AQSM_SW1PerS/mediapipe_pose.py**`\n",
    "\n",
    "MediaPipe is an open-source framework developed by Google that provides efficient, cross-platform solutions for real-time machine learning pipelines. In this project, we leverage MediaPipe's pose landmarker to capture body movements at a low computational cost compared to other models, such as OpenPose. This task uses a machine learning model on a continuous stream of images. The task outputs 33 landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9371569-2537-41ec-97e3-a34f6300e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_pose=mp.solutions.pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c2d0d-eb58-49ea-aff6-ca31a3f5dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Video Tools\n",
    "# -------------------------------------------------\n",
    "\n",
    "def get_video_info(video_path):\n",
    "    \"\"\"\n",
    "    Tool to get the number of frames in the video, the fps, and the total duration of the video in seconds. Not Necessary since \n",
    "    we already have this information above. But if starting with video then this is needed.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error opening video file\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    _, image = cap.read()\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success: \n",
    "        success,image = cap.read()\n",
    "        count += 1\n",
    "    total_frames = count\n",
    "    duration_seconds = total_frames / fps\n",
    "    cap.release()\n",
    "    frame_times = [1 / fps * i for i in range(int(fps * duration_seconds))]\n",
    "    return fps, total_frames, duration_seconds,frame_times\n",
    "\n",
    "# Section: Keypoint Handling\n",
    "# -------------------------------------------------\n",
    "    \n",
    "def getNormalizedFrameKeypoint(keypoint, dims, xmin, xmax, ymin, ymax):\n",
    "    '''\n",
    "    Normalized keypoint with respect to frame, instead of YOLOv5 bounding box\n",
    "    :Param keypoint: Keypoint coordinate for given frame\n",
    "    :Param dims: Dimension of the video frame\n",
    "    :Params xmin,xmax,ymin,ymax: YOLOv5 bounding box min/max x,y values\n",
    "    :Returns x_frame,y_frame: New normalized keypoint coordinate with respect to frame\n",
    "    '''\n",
    "    frame_width, frame_height = dims\n",
    "    x, y = keypoint[0], keypoint[1]\n",
    "    w, h = np.abs(xmax - xmin), np.abs(ymax - ymin)\n",
    "    x_unnormalized = x * w + xmin\n",
    "    y_unnormalized = y * h + ymin\n",
    "    x_frame = np.abs((x_unnormalized - frame_width) / frame_width)\n",
    "    y_frame = np.abs((y_unnormalized - frame_height) / frame_height)\n",
    "    return x_frame, y_frame\n",
    "\n",
    "# Section: YOLOv5 Box Tracking\n",
    "# -------------------------------------------------\n",
    "\n",
    "def getClass(video_name):\n",
    "    '''\n",
    "    Returns YOLOv5 class that will be used on video for each subject\n",
    "    '''\n",
    "    if '001' in video_name:\n",
    "        return 0\n",
    "    elif '002' in video_name:\n",
    "        return 1\n",
    "    elif '003' in video_name:\n",
    "        return 2\n",
    "    elif '004' in video_name:\n",
    "        return 3\n",
    "    elif '005' in video_name:\n",
    "        return 4\n",
    "    elif '006' in video_name:\n",
    "        return 5\n",
    "    else: \n",
    "        return 6\n",
    "\n",
    "def create_kalman_filter(fps):\n",
    "    dt = 1 / fps  # Time step\n",
    "    \n",
    "    kf = KalmanFilter(dim_x=8, dim_z=4)\n",
    "    \n",
    "    # State Transition Matrix (F)\n",
    "    kf.F = np.array([\n",
    "        [1, 0, 0, 0, dt, 0,  0,  0],\n",
    "        [0, 1, 0, 0, 0,  dt, 0,  0],\n",
    "        [0, 0, 1, 0, 0,  0,  dt, 0],\n",
    "        [0, 0, 0, 1, 0,  0,  0,  dt],\n",
    "        [0, 0, 0, 0, 1,  0,  0,  0],\n",
    "        [0, 0, 0, 0, 0,  1,  0,  0],\n",
    "        [0, 0, 0, 0, 0,  0,  1,  0],\n",
    "        [0, 0, 0, 0, 0,  0,  0,  1]\n",
    "    ])\n",
    "    \n",
    "    # Measurement Matrix (H)\n",
    "    kf.H = np.array([\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Process Noise Covariance (Q)\n",
    "    kf.Q *= 0.01\n",
    "    \n",
    "    # Measurement Noise Covariance (R)\n",
    "    kf.R *= 1.0\n",
    "    \n",
    "    # State Covariance Matrix (P)\n",
    "    kf.P *= 10.0\n",
    "    \n",
    "    return kf\n",
    "\n",
    "def update_kalman_filter(kf, measurement):\n",
    "    # Predict the next state\n",
    "    kf.predict()\n",
    "    \n",
    "    # Update with the current measurement (YOLOv5 bounding box)\n",
    "    kf.update(measurement)\n",
    "    \n",
    "    corrected_state = kf.x\n",
    "    return corrected_state[:4]  # Return only position and size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42f30e-22e9-4387-a892-8538bd521298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_mediapipe_file(filepath, times, fps, model, create_video = True):\n",
    "    \"\"\"\n",
    "    Using MediaPipe and YOLOv5, we get all (x,y) keypoint coordinates that will automatically be saved to a .csv file\n",
    "    :Param filepath: Video file\n",
    "    :Param times: Array of time values at each frame in video\n",
    "    :Param fps: fps of video\n",
    "    :Param model: YOLOv5 model used\n",
    "    \"\"\"\n",
    "    \n",
    "    basename = os.path.basename(filepath)  \n",
    "    # Remove the .avi suffix\n",
    "    cleaned_name = os.path.splitext(basename)[0] \n",
    "    output_video = f'{cleaned_name}_mp.avi'\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    dims = (frame_width,frame_height)\n",
    "    if create_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output_video, fourcc, fps, (frame_width,frame_height))\n",
    "\n",
    "    #Decide what child we need to track for the YOLOv5 model\n",
    "    cls = getClass(filepath)\n",
    "    \n",
    "    #For tracking centroids of current frame and previous frame\n",
    "    kf = create_kalman_filter(fps)\n",
    "\n",
    "    data = []\n",
    "    frame_count = 0\n",
    "\n",
    "    # Total number of joints in Mediapipe Pose\n",
    "    total_joints = len(mp_pose.PoseLandmark)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5,model_complexity=2) as pose:\n",
    "        for timestamp in times:\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, timestamp * 1000)\n",
    "            ret, frame = cap.read()\n",
    "            frame_data = {'frame': frame_count}\n",
    "            \n",
    "            if not ret:\n",
    "                for idx in range(total_joints):\n",
    "                    frame_data[f'joint_{idx}_x'] = np.nan\n",
    "                    frame_data[f'joint_{idx}_y'] = np.nan\n",
    "                    frame_data[f'joint_{idx}_visibility'] = np.nan\n",
    "                data.append(frame_data)\n",
    "                continue\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            \n",
    "            #Run YOLOv5 on the frame\n",
    "            results = model(image)\n",
    "            annotated_frame = results.render()[0]\n",
    "            \n",
    "            current_bbox = None\n",
    "\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            #Obtain the index of the most confident bounding box for the child\n",
    "            detection_index = 0\n",
    "            max_confidence = 0\n",
    "            for idx, det in enumerate(results.xyxy[0].tolist()):\n",
    "                xmin, ymin, xmax, ymax, confidence, clas = det\n",
    "                if clas == cls and confidence >= max_confidence:\n",
    "                    max_confidence = confidence\n",
    "                    detection_index = idx\n",
    "                    \n",
    "            MARGIN = 15\n",
    "            detected = False\n",
    "            for idx2, det in enumerate(results.xyxy[0].tolist()):\n",
    "                if idx2 == detection_index:\n",
    "                    xmin, ymin, xmax, ymax, confidence, clas = det\n",
    "                    if clas == cls:\n",
    "                        current_bbox = [xmin, ymin, xmax, ymax]\n",
    "            \n",
    "                        # Convert to [x_center, y_center, width, height] for Kalman Filter\n",
    "                        x_center = (xmin + xmax) / 2\n",
    "                        y_center = (ymin + ymax) / 2\n",
    "                        width = xmax - xmin\n",
    "                        height = ymax - ymin\n",
    "                        yolo_bbox = [x_center, y_center, width, height]\n",
    "                        \n",
    "                        # Smooth bounding box with Kalman Filter\n",
    "                        smoothed_bbox = update_kalman_filter(kf, np.array(yolo_bbox))\n",
    "                        xmin = smoothed_bbox[0].item() - smoothed_bbox[2].item() / 2\n",
    "                        ymin = smoothed_bbox[1].item() - smoothed_bbox[3].item() / 2\n",
    "                        xmax = smoothed_bbox[0].item() + smoothed_bbox[2].item() / 2\n",
    "                        ymax = smoothed_bbox[1].item() + smoothed_bbox[3].item() / 2\n",
    "                        \n",
    "                        x_min = int(max(0, xmin - MARGIN))\n",
    "                        y_min = int(max(0, ymin - MARGIN))\n",
    "                        x_max = int(min(image.shape[1], xmax + MARGIN))\n",
    "                        y_max = int(min(image.shape[0], ymax + MARGIN))\n",
    "                       \n",
    "                        cropped_frame = frame[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "                        cropped_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
    "                        #Run MediaPipe within the bounding box\n",
    "                        results = pose.process(cropped_rgb)\n",
    "\n",
    "                        if results.pose_landmarks:\n",
    "                            # Pose detected, extract landmark data\n",
    "                            for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                                x, y = landmark.x, landmark.y\n",
    "                                #Re-normalize with respect to frame\n",
    "                                x_new, y_new = getNormalizedFrameKeypoint([x,y],dims,x_min,x_max,y_min,y_max)\n",
    "                                frame_data[f'joint_{idx}_x'] = x_new\n",
    "                                frame_data[f'joint_{idx}_y'] = y_new\n",
    "                                frame_data[f'joint_{idx}_visibility'] = landmark.visibility\n",
    "                                \n",
    "                            mp_drawing.draw_landmarks(cropped_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                                      mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                                      mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "                        else:\n",
    "                            # Pose not detected, fill with NaN\n",
    "                            for idx in range(total_joints):\n",
    "                                frame_data[f'joint_{idx}_x'] = np.nan\n",
    "                                frame_data[f'joint_{idx}_y'] = np.nan\n",
    "                                frame_data[f'joint_{idx}_visibility'] = np.nan\n",
    "                    \n",
    "                        data.append(frame_data)\n",
    "            \n",
    "                        detected = True\n",
    "                        break\n",
    "\n",
    "            if not detected:\n",
    "                for idx in range(total_joints):\n",
    "                    frame_data[f'joint_{idx}_x'] = np.nan\n",
    "                    frame_data[f'joint_{idx}_y'] = np.nan\n",
    "                    frame_data[f'joint_{idx}_visibility'] = np.nan\n",
    "                data.append(frame_data)\n",
    "            if create_video:\n",
    "                out.write(frame)\n",
    "            \n",
    "            if cv2.waitKey(29) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "            frame_count += 1\n",
    "    if create_video:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f'MPdata/{cleaned_name}.csv', index=False) #Ensure you have made directory called MPdata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747789ca-1a71-4394-8e5d-a1e970d29d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is necessary if you are using a windows system to not get any errors\n",
    "'''\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Force the use of WindowsPath, this is for use on Windows\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c0b0c-62dd-4e78-b856-95e0dac2d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#Example Usage \n",
    "model_path = get_data_path(\"YOLOv5l/yolov5/runs/train/exp/weights\", \"YOLOv5l_transfer.pt\")\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
    "\n",
    "video_path = '001-01-18-08.avi'\n",
    "\n",
    "fps, total_frames, duration_seconds, frame_times = get_video_info(video_path)\n",
    "\n",
    "create_mediapipe_file(video_path, frame_times, fps, model, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08933063-45d3-4a66-b9aa-5fbb3d20471f",
   "metadata": {},
   "source": [
    "This marks the end of the processing of the publically available data from Goodwin et. al. For the remainder of the tutorial, we use the `*.pkl*` file which contains all of the data we collected above into one easy-to-use dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd32f4-4859-4807-8826-148f2c80461b",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The following functions are located in **`AQSM_SW1PerS/utils/data_processing.py`**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2dd48-de76-4619-b908-1a37cf991aa1",
   "metadata": {},
   "source": [
    "###  **Load the `.pkl` file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434f233-7e85-413c-b2d8-cbe78196f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from ATSM_SW1PerS.utils.paths import get_data_path\n",
    "\n",
    "pkl_file = get_data_path(\"dataset.pkl\")\n",
    "\n",
    "def open_pickle(pkl_file):\n",
    "    '''\n",
    "    Loads the .pkl file\n",
    "    '''\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    return loaded_data\n",
    "\n",
    "data = open_pickle(pkl_file = pkl_file)\n",
    "\n",
    "index = 0\n",
    "for entry in data:  # First video entry as example\n",
    "   \n",
    "    print(f\"Video Name: {entry['name']}, Index: {index}\")\n",
    "    index += 1\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cfc48-ea0d-4ce8-89ee-82aed10f127e",
   "metadata": {},
   "source": [
    "###  **Segmenting Videos into Overlapping Time Blocks**\n",
    "The function below is responsible for segmenting videos into overlapping time blocks, ensuring that each segment retains its associated annotations.  \n",
    "\n",
    "These time blocks will be individually analyzed in later stages of the pipeline to extract movement patterns and compute persistence-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a3d24-6a21-4b9d-8736-927faee30d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "    \n",
    "def segment_video(entry, segment_size):\n",
    "    '''    \n",
    "    Tool for segmenting video into a fixed window size and annotating the segments\n",
    "    \n",
    "    :Param filepath: entry of .pkl file\n",
    "    :Param segment_size: Desired segmentation size\n",
    "    :Return segments: Array of time intervals based on segment size\n",
    "    :Return fps, frame times, segments, and their annotations\n",
    "    '''\n",
    "    fps, frame_count, duration = entry['fps'], entry['frame_count'], entry['duration']\n",
    "    frame_times = [1 / fps * i for i in range(int(fps * duration))]\n",
    "  \n",
    "    step_size = 1/fps #In seconds\n",
    "  \n",
    "    segments = []\n",
    "    annotated_segments = []\n",
    "    \n",
    "    start_time = frame_times[0]\n",
    "    i = start_time\n",
    "    while i <= np.max(frame_times) - segment_size:\n",
    "        lower_bound = i\n",
    "        upper_bound = i + segment_size\n",
    "        x = np.array([lower_bound,upper_bound])\n",
    "        segments.append(x)\n",
    "        i += step_size\n",
    "    segments = np.vstack(segments)\n",
    "        \n",
    "    frame_annotations = entry['annotations']\n",
    "    \n",
    "    for segment in segments:\n",
    "        segment_start = np.min(segment)\n",
    "        segment_end = np.max(segment)\n",
    "        frame_start, frame_end = int(fps* segment_start), int(fps * segment_end)\n",
    "        interval_annos = np.array(frame_annotations[frame_start:frame_end])\n",
    "        interval_annos_flat = np.ravel(interval_annos)  # Flatten to 1D array\n",
    "        # Count occurrences\n",
    "        counts = Counter(interval_annos_flat.tolist())\n",
    "        # Access counts\n",
    "        num_normals = int(counts[0])\n",
    "        num_rocks = int(counts[1])\n",
    "        num_flaps = int(counts[2])\n",
    "        num_flap_rock = int(counts[3])\n",
    "        total_counts = num_normals + num_rocks + num_flaps + num_flap_rock\n",
    "        # Default annotation\n",
    "        annotation = 0\n",
    "        \n",
    "        # Ensure 100% Overlap\n",
    "        if num_rocks / total_counts == 1:\n",
    "            annotation = 1\n",
    "        elif num_flaps / total_counts == 1:\n",
    "            annotation = 2\n",
    "        elif num_flap_rock / total_counts == 1:\n",
    "            annotation = 3\n",
    "        elif (0 < num_rocks / total_counts < 1) or \\\n",
    "             (0 < num_flaps / total_counts < 1) or \\\n",
    "             (0 < num_flap_rock / total_counts < 1):\n",
    "            annotation = -1  \n",
    "\n",
    "        # Append the determined annotation\n",
    "        annotated_segments.append(annotation)\n",
    "        \n",
    "    annotated_segments = np.vstack(annotated_segments)\n",
    "    \n",
    "    return fps, frame_times, segments, annotated_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef898f-da3f-4f7e-a084-cf282569f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entry = data[13] \n",
    "fps, frame_times, segments, annotated_segments = segment_video(entry, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8133e6-9f53-4aab-b0d7-82ce8c732479",
   "metadata": {},
   "source": [
    "##  MediaPipe & YOLOv5 Pose Estimation  \n",
    "\n",
    "The function for running MediaPipe on a video can be found in **`AQSM_SW1PerS/mediapipe_pose.py`**. We use the extracted keypoints from the .pkl file.\n",
    "\n",
    "###  **Extracting Keypoints from Segmented Video**\n",
    "Now that we have the segmented video, we will extract the same keypoints used throughout the repository. These keypoints correspond to specific body landmarks and are essential for motion analysis.  \n",
    "\n",
    "###  **Overview of MediaPipe Pose Estimation**\n",
    "[MediaPipe](https://developers.google.com/mediapipe) is an open-source framework developed by Google that provides efficient, cross-platform solutions for real-time machine learning pipelines.  \n",
    "\n",
    "In this project, we use MediaPipe's Pose Landmarker to capture body movements efficiently, offering a lower computational cost compared to models like OpenPose. The pose estimation model processes a continuous stream of images and outputs 33 key body landmarks.  \n",
    "\n",
    "\n",
    "| Index | 0  | 1                | 2          | 3                | 4                | 5          | 6                | 7         | 8         | 9         | 10        |\n",
    "|-------|----|-----------------|------------|-----------------|-----------------|------------|-----------------|----------|----------|----------|----------|\n",
    "| **Landmark** | Nose | Left eye (inner) | Left eye | Left eye (outer) | Right eye (inner) | Right eye | Right eye (outer) | Left ear | Right ear | Mouth (left) | Mouth (right) |\n",
    "\n",
    "| Index | 11  | 12            | 13          | 14          | 15         | 16         | 17         | 18         | 19         | 20         | 21         | 22         |\n",
    "|-------|----|--------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n",
    "| **Landmark** | Left shoulder | Right shoulder | Left elbow | Right elbow | Left wrist | Right wrist | Left pinky | Right pinky | Left index | Right index | Left thumb | Right thumb |\n",
    "\n",
    "| Index | 23  | 24       | 25      | 26      | 27        | 28        | 29        | 30        | 31            | 32            |\n",
    "|-------|----|---------|--------|--------|----------|----------|----------|----------|--------------|--------------|\n",
    "| **Landmark** | Left hip | Right hip | Left knee | Right knee | Left ankle | Right ankle | Left heel | Right heel | Left foot  | Right foot  |\n",
    "\n",
    "\n",
    "### Keypoint Processing: Smoothing & Occlusion Handling  \n",
    "\n",
    "The following functions are designed to further process extracted keypoints, reducing sudden jumps and handling occlusions for improved motion tracking.\n",
    "\n",
    "\n",
    "These refinements ensure higher accuracy in movement detection and thus better feature extraction for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720ec13-3abf-4bd9-acc3-402ebaaf8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def getChest(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    Computes the chest position as the average of three keypoints.\n",
    "    \n",
    "    Parameters:\n",
    "      p1, p2, p3 (np.ndarray): Arrays of shape (n_frames, 3) containing x, y, z values.\n",
    "    \n",
    "    Returns:\n",
    "      np.ndarray: Array of shape (n_frames, 2) with the (x, y) chest positions.\n",
    "    \"\"\"\n",
    "    x1, y1 = p1[:, 0], p1[:, 1]\n",
    "    x2, y2 = p2[:, 0], p2[:, 1]\n",
    "    x3, y3 = p3[:, 0], p3[:, 1]\n",
    "    midpoint = [(x1 + x2 + x3) / 3, (y1 + y2 + y3) / 3]\n",
    "    return np.column_stack(midpoint)\n",
    "\n",
    "def compute_joint_angle(x1, y1, x2, y2, x3, y3):\n",
    "    \"\"\"\n",
    "    Computes the angle between two vectors: (x1,y1)->(x2,y2) and (x2,y2)->(x3,y3).\n",
    "    \n",
    "    Returns:\n",
    "      float: Angle in degrees. Returns NaN if any segment has zero length.\n",
    "    \"\"\"\n",
    "    v1 = np.array([x1 - x2, y1 - y2])\n",
    "    v2 = np.array([x3 - x2, y3 - y2])\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return np.nan\n",
    "    angle = np.arccos(np.clip(np.dot(v1, v2) / (norm_v1 * norm_v2), -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def filter_visibility(keypoints, frame_times, keypoint_index, do_wrists=False,\n",
    "                      wrist_index=None, elbow_index=None, visibility_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Filters out frames with low visibility keypoints.\n",
    "    \n",
    "    Parameters:\n",
    "      keypoints (list or np.array): List of per-frame keypoints; each frame is an iterable\n",
    "                                    of keypoint entries, where each entry is [x, y, visibility].\n",
    "      frame_times (list): Timestamps for each frame.\n",
    "      keypoint_index (int): Index of the keypoint to filter when do_wrists is False.\n",
    "      do_wrists (bool): If True, filters both wrist and elbow keypoints.\n",
    "      wrist_index (int, optional): Index of the wrist keypoint (required if do_wrists is True).\n",
    "      elbow_index (int, optional): Index of the elbow keypoint (required if do_wrists is True).\n",
    "      visibility_threshold (float): Threshold below which a keypoint is considered low visibility.\n",
    "      \n",
    "    Returns:\n",
    "      If do_wrists is False:\n",
    "        list: Frame times where the keypoint at keypoint_index is below the threshold.\n",
    "      If do_wrists is True:\n",
    "        tuple: (wrist_bad_frames, elbow_bad_frames)\n",
    "    \"\"\"\n",
    "    if do_wrists:\n",
    "        if wrist_index is None or elbow_index is None:\n",
    "            raise ValueError(\"wrist_index and elbow_index must be provided when do_wrists=True.\")\n",
    "        wrist_bad_frames = []\n",
    "        elbow_bad_frames = []\n",
    "        for idx, frame in enumerate(keypoints):\n",
    "            if frame[wrist_index][2] < visibility_threshold:\n",
    "                wrist_bad_frames.append(frame_times[idx])\n",
    "            if frame[elbow_index][2] < visibility_threshold:\n",
    "                elbow_bad_frames.append(frame_times[idx])\n",
    "        return wrist_bad_frames, elbow_bad_frames\n",
    "    else:\n",
    "        bad_frames = []\n",
    "        for idx, frame in enumerate(keypoints):\n",
    "            if frame[keypoint_index][2] < visibility_threshold:\n",
    "                bad_frames.append(frame_times[idx])\n",
    "        return bad_frames\n",
    "\n",
    "def getAccel(values, fps, n_dim = 2):\n",
    "    \"\"\"\n",
    "    Computes the second-order central derivative of the time series,\n",
    "    with forward and backward differences applied at the boundaries.\n",
    "    \"\"\"\n",
    "    h = 1 / fps\n",
    "    if n_dim == 2:\n",
    "        f_x = values[:, 0]\n",
    "        f_y = values[:, 1]\n",
    "        \n",
    "        f_x_second_deriv = (f_x[2:] - 2 * f_x[1:-1] + f_x[:-2]) / h**2\n",
    "        f_x_second_deriv_lower_bound = (f_x[1] - 2 * f_x[0] + f_x[2]) / h**2  \n",
    "        f_x_second_deriv_upper_bound = (f_x[-3] - 2 * f_x[-2] + f_x[-1]) / h**2  \n",
    "    \n",
    "        f_y_second_deriv = (f_y[2:] - 2 * f_y[1:-1] + f_y[:-2]) / h**2\n",
    "        f_y_second_deriv_lower_bound = (f_y[1] - 2 * f_y[0] + f_y[2]) / h**2  \n",
    "        f_y_second_deriv_upper_bound = (f_y[-3] - 2 * f_y[-2] + f_y[-1]) / h**2\n",
    "      \n",
    "        f_x_second_derivative = np.concatenate(([f_x_second_deriv_lower_bound], f_x_second_deriv, [f_x_second_deriv_upper_bound]))\n",
    "        f_y_second_derivative = np.concatenate(([f_y_second_deriv_lower_bound], f_y_second_deriv, [f_y_second_deriv_upper_bound]))\n",
    "\n",
    "        return np.column_stack((f_x_second_derivative, f_y_second_derivative))\n",
    "    else:\n",
    "        f_x = values\n",
    "        \n",
    "        f_x_second_deriv = (f_x[2:] - 2 * f_x[1:-1] + f_x[:-2]) / h**2\n",
    "        f_x_second_deriv_lower_bound = (f_x[1] - 2 * f_x[0] + f_x[2]) / h**2  \n",
    "        f_x_second_deriv_upper_bound = (f_x[-3] - 2 * f_x[-2] + f_x[-1]) / h**2  \n",
    "\n",
    "        return np.concatenate(([f_x_second_deriv_lower_bound], f_x_second_deriv, [f_x_second_deriv_upper_bound]))\n",
    "\n",
    "\n",
    "def interprolate_missing_frames(df, col1, col2, method=\"cubic\"):\n",
    "    \"\"\"\n",
    "    Interpolates the two columns in the DataFrame using the specified method,\n",
    "    then fills any leading NaNs with the first valid value (backfill) and \n",
    "    trailing NaNs with the last valid value (forward fill).\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        col1 (str): The name of the first column.\n",
    "        col2 (str): The name of the second column.\n",
    "        method (str): Interpolation method (default \"cubic\").\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Interpolate the entire series without a limit.\n",
    "    df[col1] = df[col1].interpolate(method=method)\n",
    "    df[col2] = df[col2].interpolate(method=method)\n",
    "\n",
    "    # --- Handle leading NaNs: backfill from the first valid value ---\n",
    "    # For col1:\n",
    "    if pd.isna(df[col1].iloc[0]):\n",
    "        first_valid_idx = df[col1].first_valid_index()\n",
    "        if first_valid_idx is not None:\n",
    "            # Fill all rows up to and including the first valid index\n",
    "            df.loc[:first_valid_idx, col1] = df.loc[first_valid_idx, col1]\n",
    "            \n",
    "    # For col2:\n",
    "    if pd.isna(df[col2].iloc[0]):\n",
    "        first_valid_idx = df[col2].first_valid_index()\n",
    "        if first_valid_idx is not None:\n",
    "            df.loc[:first_valid_idx, col2] = df.loc[first_valid_idx, col2]\n",
    "    \n",
    "    # --- Handle trailing NaNs: forward fill from the last valid value ---\n",
    "    # For col1:\n",
    "    if pd.isna(df[col1].iloc[-1]):\n",
    "        last_valid_idx = df[col1].last_valid_index()\n",
    "        if last_valid_idx is not None:\n",
    "            df.loc[last_valid_idx:, col1] = df.loc[last_valid_idx, col1]\n",
    "            \n",
    "    # For col2:\n",
    "    if pd.isna(df[col2].iloc[-1]):\n",
    "        last_valid_idx = df[col2].last_valid_index()\n",
    "        if last_valid_idx is not None:\n",
    "            df.loc[last_valid_idx:, col2] = df.loc[last_valid_idx, col2]\n",
    "\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_keypoints(entry, keypoint_index, frame_times, fps, do_wrists=False,\n",
    "                      elbow_index=None, shoulder_index=None):\n",
    "    \"\"\"\n",
    "    Extracts and cleans keypoint (x, y) data.\n",
    "    \n",
    "    If do_wrists is False, the function returns cleaned (x, y) positions for the specified keypoint.\n",
    "    If do_wrists is True, it also extracts elbow and shoulder points to compute elbow flexion angles,\n",
    "    angular velocity, and acceleration for wrist-specific processing.\n",
    "    \n",
    "    Parameters:\n",
    "      entry (dict): Contains 'keypoints' (list of per-frame keypoint data).\n",
    "      keypoint_index (int): Index for the target keypoint (or wrist if do_wrists is True).\n",
    "      frame_times (list): Timestamps for each frame.\n",
    "      fps (float): Frames per second.\n",
    "      do_wrists (bool): Flag for wrist-specific processing.\n",
    "      elbow_index (int, optional): Required if do_wrists is True.\n",
    "      shoulder_index (int, optional): Required if do_wrists is True.\n",
    "      \n",
    "    Returns:\n",
    "      np.ndarray: Cleaned (x, y) positions. If do_wrists is True, returns wrist positions.\n",
    "    \"\"\"\n",
    "    keypoints = entry['keypoints']\n",
    "    \n",
    "    if do_wrists:\n",
    "        \n",
    "        # Get bad visibility frames for wrist and elbow\n",
    "        wrist_bad_frames, elbow_bad_frames = filter_visibility(keypoints, frame_times, keypoint_index, do_wrists=True, wrist_index=keypoint_index, elbow_index=elbow_index, visibility_threshold=0.2)\n",
    "        # Extract (x,y) data for wrist, elbow, and shoulder from each frame.\n",
    "        keypoint_array = [[[x, y] for x, y, _ in frame] for frame in keypoints]\n",
    "        wrist_points, elbow_points, shoulder_points = [], [], []\n",
    "        for frame in keypoint_array:\n",
    "            if (keypoint_index < len(frame) and elbow_index < len(frame) and shoulder_index < len(frame)):\n",
    "                wrist_points.append(frame[keypoint_index])\n",
    "                elbow_points.append(frame[elbow_index])\n",
    "                shoulder_points.append(frame[shoulder_index])\n",
    "            else:\n",
    "                # In production code you might want to handle this differently.\n",
    "                continue\n",
    "        \n",
    "        wrist_points = np.vstack(wrist_points)\n",
    "        elbow_points = np.vstack(elbow_points)\n",
    "        shoulder_points = np.vstack(shoulder_points)\n",
    "        \n",
    "        # Build a DataFrame for wrist processing.\n",
    "        df = pd.DataFrame({\n",
    "            \"Time\": frame_times[:len(wrist_points)],\n",
    "            \"X_Wrist\": wrist_points[:, 0],\n",
    "            \"Y_Wrist\": wrist_points[:, 1],\n",
    "            \"X_Elbow\": elbow_points[:, 0],\n",
    "            \"Y_Elbow\": elbow_points[:, 1],\n",
    "            \"X_Shoulder\": shoulder_points[:, 0],\n",
    "            \"Y_Shoulder\": shoulder_points[:, 1],\n",
    "        })\n",
    "        \n",
    "        # Mark low visibility elbow frames as NaN.\n",
    "        df.loc[df[\"Time\"].isin(elbow_bad_frames), [\"X_Elbow\", \"Y_Elbow\"]] = np.nan\n",
    "        \n",
    "        df = interprolate_missing_frames(df, \"X_Elbow\", \"Y_Elbow\")\n",
    "\n",
    "        # Compute elbow flexion angle.\n",
    "        df[\"Elbow_Flexion_Angle\"] = df.apply(lambda row: compute_joint_angle(\n",
    "            row[\"X_Shoulder\"], row[\"Y_Shoulder\"],\n",
    "            row[\"X_Elbow\"], row[\"Y_Elbow\"],\n",
    "            row[\"X_Wrist\"], row[\"Y_Wrist\"]\n",
    "        ), axis=1)\n",
    "        \n",
    "        angles = df[\"Elbow_Flexion_Angle\"].values\n",
    "        angular_acceleration = getAccel(angles, fps, n_dim = 1)\n",
    "        df[\"Angular_Acceleration\"] = angular_acceleration\n",
    "        \n",
    "        # Flag frames with extreme angular acceleration.\n",
    "        df.loc[df[\"Time\"].isin(wrist_bad_frames), \"Angular_Acceleration\"] = 10000\n",
    "        \n",
    "        bad_frames = df[np.abs(df[\"Angular_Acceleration\"]) > 500].index #500, and 5000, 150\n",
    "        df[\"Valid\"] = True\n",
    "        df.loc[bad_frames, \"Valid\"] = False\n",
    "        df.loc[bad_frames, [\"X_Wrist\", \"Y_Wrist\"]] = np.nan\n",
    "\n",
    "        # Interpolate and smooth wrist coordinates.\n",
    "        df = interprolate_missing_frames(df, \"X_Wrist\", \"Y_Wrist\")\n",
    "        \n",
    "        sigma = 0.5\n",
    "        df[\"X_Wrist\"] = scipy.ndimage.gaussian_filter1d(df[\"X_Wrist\"], sigma=sigma)\n",
    "        df[\"Y_Wrist\"] = scipy.ndimage.gaussian_filter1d(df[\"Y_Wrist\"], sigma=sigma)\n",
    "        \n",
    "        return df[[\"X_Wrist\", \"Y_Wrist\"]].to_numpy()\n",
    "    \n",
    "    else:\n",
    "        # For non-wrist processing, filter visibility and extract the desired keypoint.\n",
    "        bad_visibility_frames = filter_visibility(keypoints, frame_times, keypoint_index)\n",
    "        keypoint_array = [[[x, y] for x, y, _ in frame] for frame in keypoints]\n",
    "        extracted = []\n",
    "        for frame in keypoint_array:\n",
    "            if keypoint_index < len(frame):\n",
    "                extracted.append(frame[keypoint_index])\n",
    "            else:\n",
    "                continue\n",
    "        extracted = np.vstack(extracted)\n",
    "        \n",
    "        # Compute acceleration using external function getAccel.\n",
    "        acceleration = getAccel(extracted, fps)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            \"Time\": frame_times[:len(extracted)],\n",
    "            \"X\": extracted[:, 0],\n",
    "            \"Y\": extracted[:, 1],\n",
    "        })\n",
    "        df[\"Acceleration X\"] = acceleration[:, 0]\n",
    "        df[\"Acceleration Y\"] = acceleration[:, 1]\n",
    "        \n",
    "        # Set acceleration of keypoints with low visibility to a high value.\n",
    "        df.loc[df[\"Time\"].isin(bad_visibility_frames), [\"Acceleration X\", \"Acceleration Y\"]] = 10000\n",
    "        \n",
    "        # Flag bad frames based on acceleration thresholds.\n",
    "        bad_frames = df[(np.abs(df[\"Acceleration X\"]) > 4) | (np.abs(df[\"Acceleration Y\"]) > 10)].index\n",
    "        df[\"Valid\"] = True\n",
    "        df.loc[bad_frames, \"Valid\"] = False\n",
    "        df.loc[bad_frames, [\"X\", \"Y\"]] = np.nan\n",
    "        \n",
    "        # Interpolate and smooth the data.\n",
    "        df = interprolate_missing_frames(df, \"X\", \"Y\")\n",
    "        \n",
    "        sigma = 0.5\n",
    "        df[\"X\"] = scipy.ndimage.gaussian_filter1d(df[\"X\"], sigma=sigma)\n",
    "        df[\"Y\"] = scipy.ndimage.gaussian_filter1d(df[\"Y\"], sigma=sigma)\n",
    "        \n",
    "        return df[[\"X\", \"Y\"]].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a054b-2892-4302-9c8e-28aef296e067",
   "metadata": {},
   "source": [
    "## Keypoint Analysis: Raw Positions & Acceleration  \n",
    "\n",
    "In our pipeline, we will analyze both:  \n",
    "1. Raw keypoint positions – The direct (x, y) coordinates extracted from MediaPipe.  \n",
    "1. Acceleration representations – Primarily estimated from the second-order foward difference. The foward and backwards differences are computed on the boundaries to maintain the same size. \n",
    "\n",
    "This dual-analysis approach ensures a comprehensive understanding of movement patterns in each segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a6392-01a6-4f4f-a467-67781c239eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "head = extract_keypoints(entry, 0, frame_times, fps)\n",
    "lshoulder = extract_keypoints(entry, 11, frame_times, fps)\n",
    "rshoulder = extract_keypoints(entry, 12, frame_times, fps)\n",
    "rwrist = extract_keypoints(entry, 16, frame_times, fps, do_wrists=True, elbow_index=14, shoulder_index=12)\n",
    "lwrist = extract_keypoints(entry, 15, frame_times, fps, do_wrists=True, elbow_index=13, shoulder_index=11)\n",
    "\n",
    "chest = getChest(head, lshoulder, rshoulder)\n",
    "\n",
    "lshoulder_accel = getAccel(lshoulder,fps)\n",
    "rshoulder_accel = getAccel(rshoulder,fps)\n",
    "\n",
    "rwrist_accel = getAccel(rwrist,fps)\n",
    "lwrist_accel = getAccel(lwrist,fps)\n",
    "\n",
    "head_accel = getAccel(head,fps)\n",
    "chest_accel = getAccel(chest,fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a5286-2c03-4eb5-bfe7-04d597ab7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Turn Keypoints into Spline Representation for the Main Algorithm to Interpolate Values\n",
    "'''\n",
    "\n",
    "#Raw keypoint positions\n",
    "\n",
    "h_x, h_y = CubicSpline(frame_times,head[:,0]), CubicSpline(frame_times,head[:,1])\n",
    "\n",
    "r_x, r_y = CubicSpline(frame_times,rwrist[:,0]), CubicSpline(frame_times,rwrist[:,1])\n",
    "\n",
    "l_x, l_y = CubicSpline(frame_times,lwrist[:,0]), CubicSpline(frame_times,lwrist[:,1])\n",
    "\n",
    "rs_x, rs_y = CubicSpline(frame_times,rshoulder[:,0]), CubicSpline(frame_times,rshoulder[:,1])\n",
    "\n",
    "ls_x, ls_y = CubicSpline(frame_times,lshoulder[:,0]), CubicSpline(frame_times,lshoulder[:,1])\n",
    "\n",
    "c_x, c_y = CubicSpline(frame_times,chest[:,0]), CubicSpline(frame_times,chest[:,1])\n",
    "\n",
    "#Acceleration representation of keypoint movement\n",
    "\n",
    "h_x_a, h_y_a = CubicSpline(frame_times,head_accel[:,0]), CubicSpline(frame_times,head_accel[:,1])\n",
    "\n",
    "r_x_a, r_y_a = CubicSpline(frame_times,rwrist_accel[:,0]), CubicSpline(frame_times,rwrist_accel[:,1])\n",
    "\n",
    "l_x_a, l_y_a = CubicSpline(frame_times,lwrist_accel[:,0]), CubicSpline(frame_times,lwrist_accel[:,1])\n",
    "\n",
    "rs_x_a, rs_y_a = CubicSpline(frame_times,rshoulder_accel[:,0]), CubicSpline(frame_times,rshoulder_accel[:,1])\n",
    "\n",
    "ls_x_a, ls_y_a = CubicSpline(frame_times,lshoulder_accel[:,0]), CubicSpline(frame_times,lshoulder_accel[:,1])\n",
    "\n",
    "c_x_a, c_y_a = CubicSpline(frame_times,chest_accel[:,0]), CubicSpline(frame_times,chest_accel[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16e06d-e61b-4710-8e24-d552a8c2767c",
   "metadata": {},
   "source": [
    "#  SW1PerS Algorithm  \n",
    "\n",
    "The following definitions can be found in: **`AQSM_SW1PerS/SW1PerS.py`**  \n",
    "\n",
    "###  **Overview**\n",
    "The **Sliding Windows and 1-Persistence Scoring  (SW1PerS)** algorithm is designed to analyze motion periodicity using a combination of sliding windows and persistent homology.  \n",
    "\n",
    "Below is the pipeline for obtaining periodicity scores for each video segment.\n",
    "\n",
    "---\n",
    "\n",
    "## **SW1PerS Algorithm: Computing Periodicity Scores**\n",
    "1. **Preprocessing** – Extract keypoints and clean motion data.  \n",
    "2. **Period Estimation** – Compute dominant movement frequency.  \n",
    "3. **Optimizing Embedding Parameters** – Find the best $d$ and $\\tau$ for sliding window embedding.\n",
    "4.  **Sliding Window Embedding** – Convert motion data into high-dimensional point cloud.  \n",
    "5. **Persistent Homology Computation** – Extract topological features from embedded motion data.  \n",
    "6. **Scoring & Interpretation** – Extract the 10 most persistent points and compute periodicity scores.  \n",
    "\n",
    "This pipeline enables automated movement tracking and provides a quantification of recurrent signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e5ff5-75a8-46b9-b20d-ecf7e6187720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from scipy.interpolate import CubicSpline\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import multiprocessing as multi\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dc70a-3efa-4774-966d-5bd844c54e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: choose either a time value OR an annotation\n",
    "time_value = None     # set to a numeric value if you want to choose by time\n",
    "annotation = 1   # set to a string if you want to choose by annotation\n",
    "\n",
    "# Specify which occurrence you want (0 for first, 1 for second, etc.)\n",
    "occurrence = 1  # for example, choose the second occurrence\n",
    "\n",
    "if (time_value is None and annotation is None) or (time_value is not None and annotation is not None):\n",
    "    raise ValueError(\"Please provide either a time_value OR an annotation (but not both).\")\n",
    "\n",
    "if time_value is not None:\n",
    "    # Gather all matching segment indices based on time_value\n",
    "    matching_indices = [\n",
    "        i for i in range(len(segments) - 1)\n",
    "        if np.min(segments[i]) >= time_value and np.min(segments[i + 1]) != time_value\n",
    "    ]\n",
    "else:\n",
    "    # Gather all matching segment indices based on annotation\n",
    "    matching_indices = [i for i, ann in enumerate(annotated_segments) if ann == annotation]\n",
    "\n",
    "if not matching_indices:\n",
    "    raise ValueError(\"No segment found for the given criteria.\")\n",
    "\n",
    "if occurrence >= len(matching_indices):\n",
    "    raise ValueError(\"Occurrence number is out of range. There are only {} matches.\".format(len(matching_indices)))\n",
    "\n",
    "idx = matching_indices[occurrence]\n",
    "\n",
    "segment = segments[idx]\n",
    "annotated_segment = annotated_segments[idx]\n",
    "\n",
    "print(\"Chosen segment:\", segment)\n",
    "print(\"Chosen annotated segment:\", annotated_segment)\n",
    "\n",
    "\n",
    "# --- Choose a Sensor ---\n",
    "def choose_sensor(sensor='Chest'):\n",
    "    # Map sensor names to their corresponding (position, acceleration) tuples.\n",
    "    sensor_map = {\n",
    "        'Chest': ((c_x, c_y), (c_x_a, c_y_a)),\n",
    "        'LW':    ((l_x, l_y), (l_x_a, l_y_a)),\n",
    "        'RW':    ((r_x, r_y), (r_x_a, r_y_a)),\n",
    "        'LS':    ((ls_x, ls_y), (ls_x_a, ls_y_a)),\n",
    "        'RS':    ((rs_x, rs_y), (rs_x_a, rs_y_a)),\n",
    "        'Head':  ((h_x, h_y), (h_x_a, h_y_a))\n",
    "    }\n",
    "    \n",
    "    if sensor not in sensor_map:\n",
    "        raise ValueError(\"Not a valid sensor. Valid options are: \" + \", \".join(sensor_map.keys()))\n",
    "    \n",
    "    (t_x, t_y), (t_x_a, t_y_a) = sensor_map[sensor]\n",
    "    return t_x, t_y, t_x_a, t_y_a\n",
    "\n",
    "# Example usage:\n",
    "t_x, t_y, t_x_a, t_y_a = choose_sensor(sensor='LS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107a057-8ab6-4208-be73-ac6b2f4d755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setup\n",
    "'''\n",
    "\n",
    "start = np.min(segment)\n",
    "end = np.max(segment)\n",
    "\n",
    "#Interprolate more points for time window\n",
    "num_points = 1000\n",
    "t_vals = np.linspace(start,end,num_points)\n",
    "\n",
    "keypoint_x = t_x(t_vals)\n",
    "keypoint_y = t_y(t_vals)\n",
    "\n",
    "\n",
    "keypoint_x_accel = t_x_a(t_vals)\n",
    "keypoint_y_accel = t_y_a(t_vals)\n",
    "\n",
    "\n",
    "#Remove linear trends in the time series\n",
    "keypoint_x = signal.detrend(keypoint_x)\n",
    "keypoint_y = signal.detrend(keypoint_y)\n",
    "\n",
    "\n",
    "keypoint_x_accel = signal.detrend(keypoint_x_accel)\n",
    "keypoint_y_accel = signal.detrend(keypoint_y_accel)\n",
    "\n",
    "\n",
    "#Store Cubic Spline representations of accelerometers into an array. Needed for Sliding Window Computation\n",
    "f_x = CubicSpline(t_vals, keypoint_x)\n",
    "f_y = CubicSpline(t_vals, keypoint_y) \n",
    "\n",
    "\n",
    "f_x_a = CubicSpline(t_vals, keypoint_x_accel)\n",
    "f_y_a = CubicSpline(t_vals, keypoint_y_accel) \n",
    "\n",
    "\n",
    "cs1 = []\n",
    "cs1.append(f_x)\n",
    "cs1.append(f_y)\n",
    "\n",
    "\n",
    "cs2 = []\n",
    "cs2.append(f_x_a)\n",
    "cs2.append(f_y_a)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))  \n",
    "\n",
    "axes[0].plot(t_vals,keypoint_x,color='r',label = 'X')\n",
    "axes[0].plot(t_vals,keypoint_y,color='g',label = 'Y')\n",
    "\n",
    "axes[0].set_title(\"Keypoint Positions\")\n",
    "axes[0].set_xlabel(\"Time\")\n",
    "axes[0].set_ylabel(\"Normalized Position\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Second subplot\n",
    "axes[1].plot(t_vals,keypoint_x_accel,color='r',label = 'X')\n",
    "axes[1].plot(t_vals,keypoint_y_accel,color='g',label = 'Y')\n",
    "axes[1].set_title(\"Acceleration Keypoint\")\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "axes[1].set_ylabel(\"Acceleration\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c18d4b-63ab-44c3-b0e0-f8f8f7d39445",
   "metadata": {},
   "source": [
    "### Period Estimation\n",
    "\n",
    "It has been observed that when the window size $d\\tau$ approaches the length of the period of a signal $f: \\mathbb{R} \\longrightarrow \\mathbb{R}$, the roundedness of its sliding window point cloud is maximized. \n",
    "This, in turn,\n",
    "maximizes the persistence of the point fartherst from the diagonal in $dgm_1$, and hence the strength of the periodicity score computed via persistence diagrams. \n",
    "In particular, if $f$ satisfies the identity \n",
    "$$\n",
    "f(t + L) = f(t) \\quad \\text{for } L >0 \\text{ minimal and all } t,\n",
    "$$\n",
    "i.e., $f$ is $L$-periodic, then its $L$-periodicity is best captured by the persistence diagrams of its sliding window point cloud when:\n",
    "\n",
    "$$    d\\tau= \\left( \\frac{d}{d+1} \\right)L\n",
    "$$\n",
    "\n",
    "We use the Complex Discrete Fourier Transform as discussed in the *Methods* section of the manuscript to estimate $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e4b5e-ab66-46e9-8dcf-7e79f5842dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Obtaining Optimal d and tau values for Sliding Window Embedding\n",
    "'''\n",
    "\n",
    "def find_prominent_peaks(magnitude_spectrum, threshold, prominence):\n",
    "    '''\n",
    "    Tool to find the optimal d value for Sliding Windows by sorting the peaks of the DFT \n",
    "    :Param magnitude_spectrum: Array of magnitude spectrum from DFT data\n",
    "    :Param threshold: Minimum value that a peak must exceed to be considered prominent\n",
    "    :Param promincence: Measure of how much a peak stands out from the surrounding data\n",
    "    :Return sorted_peaks[:n_peaks]: Sorted array of most prominent peaks\n",
    "    '''\n",
    "    peaks, _ = find_peaks(magnitude_spectrum, height=threshold, prominence=prominence)\n",
    "    sorted_peaks = sorted(peaks, key=lambda p: -magnitude_spectrum[p])\n",
    "    cumulative_sum = np.cumsum(magnitude_spectrum[sorted_peaks])\n",
    "    total_sum = np.sum(magnitude_spectrum[sorted_peaks])\n",
    "    n_peaks = np.searchsorted(cumulative_sum, 0.7 * total_sum, side='right')\n",
    "    return sorted_peaks[:n_peaks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ffeb8-0750-461d-b357-6721203b0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4 #Time block size\n",
    "\n",
    "F1 = keypoint_x + 1j * keypoint_y\n",
    "F2 = keypoint_x_accel + 1j * keypoint_y_accel\n",
    "\n",
    "Fs = num_points/(end-start)\n",
    "\n",
    "try:\n",
    "    dft1 = np.fft.fft(F1)\n",
    "    magnitude_spectrum1 = np.abs(dft1)\n",
    "    frequencies1 = np.fft.fftfreq(len(F1), 1/Fs)\n",
    "    \n",
    "    pk1 = find_prominent_peaks(magnitude_spectrum1, 0, 0) \n",
    "    \n",
    "    peak_center1 = frequencies1[pk1][0]\n",
    "    \n",
    "    period1 = np.abs((1/peak_center1))\n",
    "\n",
    "    if period1 > window_size / 2: #Cutoff Period\n",
    "        epsilon = 1e-10  \n",
    "    \n",
    "        \n",
    "        frequencies1 = np.where(frequencies1 == 0, epsilon, frequencies1)\n",
    "    \n",
    "        cutoff_period1 = window_size / 2\n",
    "    \n",
    "        periods1 = np.abs(1/frequencies1)\n",
    "        # Apply cutoff to magnitude spectrum\n",
    "        idx = np.where(periods1 < cutoff_period1)  # Use > to zero out values above cutoff\n",
    "        magnitude_spectrum1 = magnitude_spectrum1[idx]\n",
    "        frequencies1 = frequencies1[idx]\n",
    "        pk1 = find_prominent_peaks(magnitude_spectrum1, 0, 0) \n",
    "        \n",
    "        peak_center1 = frequencies1[pk1][0]\n",
    "        period1 = np.abs((1/peak_center1))\n",
    "except:\n",
    "    period1 = 0\n",
    "    frequencies1, magnitude_spectrum1 = 0, 0\n",
    "\n",
    "try:\n",
    "    dft2 = np.fft.fft(F2)\n",
    "    magnitude_spectrum2 = np.abs(dft2)\n",
    "    frequencies2 = np.fft.fftfreq(len(F2), 1/Fs)\n",
    "    \n",
    "    pk2 = find_prominent_peaks(magnitude_spectrum2, 0, 0) \n",
    "    \n",
    "    peak_center2 = frequencies2[pk2][0]\n",
    "    \n",
    "    period2 = np.abs((1/peak_center2))\n",
    "\n",
    "    if period2 > window_size / 2: #Cutoff Period\n",
    "        epsilon = 1e-10  \n",
    "        \n",
    "        frequencies2 = np.where(frequencies2 == 0, epsilon, frequencies2)\n",
    "    \n",
    "        cutoff_period2 = window_size / 2\n",
    "    \n",
    "        periods2 = np.abs(1/frequencies2)\n",
    "        # Apply cutoff to magnitude spectrum\n",
    "        idx = np.where(periods2 < cutoff_period2)  # Use > to zero out values above cutoff\n",
    "        magnitude_spectrum2 = magnitude_spectrum2[idx]\n",
    "        frequencies2 = frequencies2[idx]\n",
    "        pk2 = find_prominent_peaks(magnitude_spectrum2, 0, 0) \n",
    "    \n",
    "        peak_center2 = frequencies2[pk2][0]\n",
    "        period2 = np.abs((1/peak_center2))\n",
    "except:\n",
    "    period2 = 0\n",
    "    frequencies2, magnitude_spectrum2 = 0, 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))  # 1 row, 2 columns\n",
    "\n",
    "axes[0].plot(frequencies1,magnitude_spectrum1)\n",
    "try:\n",
    "    axes[0].plot(frequencies1[pk1],magnitude_spectrum1[pk1],'x')\n",
    "    axes[0].vlines(peak_center1, ymin=0, ymax=np.max(magnitude_spectrum1[pk1]), colors='r', linestyles='dashed',label = 'Fundamental Freq.')\n",
    "except:\n",
    "    print('Raw frequency detection failed')\n",
    "axes[0].set_xlabel(fr'$\\omega$')\n",
    "axes[0].set_ylabel(fr'$|F(\\omega)|$')\n",
    "axes[0].set_title(\"Raw Keypoint Magnitude Spectrum\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(frequencies2,magnitude_spectrum2)\n",
    "try:\n",
    "    axes[1].plot(frequencies2[pk2],magnitude_spectrum2[pk2],'x')\n",
    "    axes[1].vlines(peak_center2, ymin=0, ymax=np.max(magnitude_spectrum2[pk2]), colors='r', linestyles='dashed',label = 'Fundamental Freq.')\n",
    "except:\n",
    "    print('Raw frequency detection failed')\n",
    "axes[1].set_xlabel(fr'$\\omega$')\n",
    "axes[1].set_ylabel(fr'$|F(\\omega)|$')\n",
    "axes[1].set_title(\"Acceleration Magnitude Spectrum\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "d = 23\n",
    "\n",
    "t1 = period1 / (d + 1)\n",
    "t2 = period2 / (d + 1)\n",
    "\n",
    "print(fr'Window Size for Raw Keypoints: {d*t1}')\n",
    "print(fr'Window Size for Acceleration: {d*t2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8aff69-d007-4f0d-b1a3-849d7a68f34c",
   "metadata": {},
   "source": [
    "### Sliding Window Embedding\n",
    "\n",
    "To recognize periodicity (e.g., in the movement of estimated keypoint locations), one can transform the problem of recurrence detection in time series data into that of shape analysis, where tools from TDA can be applied. Specifically, we use the Sliding Window Embedding, which transforms a time series into a discretized point cloud in a high-dimensional space.\n",
    "\n",
    "Let $F : \\mathbb{R} \\longrightarrow \\mathbb{R}^m$ be an $m$-dimensional time series, $m\\geq 1$, and write $F(t)=(f_1(t),...,f_m(t))$. Given parameters $d\\in \\mathbb{N}$ and $\\tau>0$, the sliding window embedding of the signal $F$ at $t \\in \\mathbb{R}$ is the matrix:\n",
    "\n",
    "$$\n",
    "SW_{d, \\tau}F(t) =\n",
    "\\begin{bmatrix}\n",
    "    f_1(t) & \\dots & f_m(t)\\\\\n",
    "    f_1(t + \\tau) & \\dots & f_m(t + \\tau) \\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    f_1(t + d\\tau) & \\dots & f_m(t + d\\tau)\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(d+1)\\times m}\n",
    "$$\n",
    "\n",
    "Moreover, given a set of time values $I\\subset \\mathbb{R}$, the sliding window point cloud of $F$ \n",
    "is the set $X = \\{SW_{d,\\tau} F(t) \\; : \\; t\\in I\\} \\subset \\mathbb{R}^{(d+1)\\times m} $.\n",
    "We will use the Euclidean (i.e., Frobenius) norm $\\|\\cdot\\|$ in $\\mathbb{R}^{(d+1)\\times m}$\n",
    "to measure pairwise distances $\\|p - p'\\|$\n",
    "between points  $p,p' \\in X$. \n",
    "\n",
    "When the signal $F$ is only observed at discrete \n",
    "time points $t_0 < t_1 < \\cdots < t_n$  \n",
    "we  extend it to   $t\\in \\mathbb{R}$, before computing its  sliding window point cloud, as follows:\n",
    "$$\n",
    "F(t) = \n",
    "\\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    F(t_0)  & \\hbox{if }\\;\\; t\\leq t_0  \\\\ \\\\\n",
    "    F(t_n) & \\hbox{if } \\;\\; t\\geq t_n \\\\ \\\\\n",
    "    F_{spline}(t) & \\hbox{if }\\;\\; t_i \\leq t \\leq t_{i+1}, \\; i = 0,\\ldots, n-1\n",
    "  \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "where $F_{spline} : \\mathbb{R} \\longrightarrow \\mathbb{R}^{m}$ is the function whose $j$-th coordinate, $j=1,\\ldots, m$, is the cubic spline interpolating the points $(t_i, f_j(t_i))$ for $i=0,\\ldots, n$.\n",
    "\n",
    "\n",
    "Dense regions in the sliding window point cloud $X\\subset \\mathbb{R}^{(d+1)\\times m}$ correspond to areas capturing the underlying shape and geometry of the object. \n",
    "More sparse regions, on the other hand,  can contribute noisy data degrading topological inference. \n",
    "To filter out this sparse data, we use a $k$-nearest neighbors density approach. \n",
    "That is, for $k\\in \\mathbb{N}$ and each $p\\in X$, let $N_k(p) \\subset X$ be the set comprised of the $k$ nearest neighbors of $p$ in $X$ using the Euclidean distance. \n",
    "To filter based on density, we estimate the density of a point  as the reciprocal of the  median distance to its  $k$-nearest neighbors: \n",
    "$$    \\rho_k(p) = \\frac{1}{\\text{median}\n",
    "    \\left(\n",
    "    \\left\\{\n",
    "    \\|p - p'\\| \\; : \\; p' \\in N_k(p)\\,\n",
    "    \\right\\}\n",
    "    \\right)}\n",
    "$$ Given the values $\\rho_k(p)$ for $p\\in X$, we let the subsample of densest points be\n",
    "$$X_s = \n",
    "\\left\\{\n",
    "p \\in X \\, : \\, \\rho_k(p) \\mbox{ is among the top $80\\%$ of values}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "``knn_density`` below is the function that acheives this filtering of the sliding window point cloud.\n",
    "\n",
    "After subsampling the point cloud, we mean-center each point individually and normalize it to enable comparisons between the periodic motions of different videos and varying ranges or intensities. \n",
    "That is: \n",
    "$$\\widetilde{X_s} = \n",
    "\\left\\{\n",
    "\\frac{p - \\mathsf{mean}(p)}{\\|p - \\mathsf{mean}(p)\\|} \\; : \\; p \\in X_s\n",
    "\\right\\}\n",
    "$$where $\\mathsf{mean}(p)\\in \\mathbb{R}^{(d+1)\\times m}$\n",
    "is the matrix with all entries equal to the average of the entries of the $(d+1)\\times m$ matrix  $p \\in X_s$. \n",
    "We remark that density thresholding in the sliding window point cloud ameliorates the impact of noisy recurrent behavior in $F$, and that mean centering helps remove linear trends from the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3259c3c-07b6-487e-b21f-004043485aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def knn_density(point_cloud, k=10): \n",
    "    '''\n",
    "    Tool to filter out sparse data in point cloud to only collect the important information\n",
    "    '''\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(point_cloud)\n",
    "    distances, indices = nbrs.kneighbors(point_cloud)\n",
    "    densities = 1 / np.nanmedian(distances, axis=1)\n",
    "    \n",
    "    sample_percentage = 0.8\n",
    "    filtered_points = int(sample_percentage * len(point_cloud))\n",
    "    \n",
    "    dense_indices = np.argsort(densities)[-filtered_points:]\n",
    "    \n",
    "    subsampled_point_cloud = point_cloud[dense_indices]\n",
    "    \n",
    "    return subsampled_point_cloud\n",
    "\n",
    "import numpy.matlib\n",
    "\n",
    "def SW_cloud_nD(cs, x_vals, tau, d, n_data, n_dims):\n",
    "    '''\n",
    "    Computes the sliding window embedding with n columns.\n",
    "    :param cs: List containing the cubic splines for f1(x),f2(x), . . . ,fn(x)\n",
    "    :param x_vals: Time values of frames segment\n",
    "    :param tau: Optimal tau value (delay)\n",
    "    :param d: Optimal embedding dimension\n",
    "    :param n_data: Number of points desired in the point cloud\n",
    "    :return SW_normalized: Point cloud from sliding window embedding, normalized\n",
    "    '''\n",
    "    t_vals = np.linspace(np.min(x_vals), np.max(x_vals) - (d * tau), n_data)\n",
    "    SW = np.zeros((n_data, (d + 1) * n_dims))\n",
    "    \n",
    "    for i, t in enumerate(t_vals):\n",
    "        for j in range(n_dims):\n",
    "            SW_f_t = cs[j](t + np.arange(0, d + 1) * tau)\n",
    "            SW[i, j * (d + 1):(j + 1) * (d + 1)] = SW_f_t\n",
    "    \n",
    "    SW = knn_density(SW, k=20)\n",
    "\n",
    "    # Subtract the mean from each row (mean centering)\n",
    "    SW_mean = np.mean(SW, axis=1)\n",
    "    SW_mean = numpy.matlib.repmat(SW_mean,  np.shape(SW)[1],1).T\n",
    "    SW_centered = SW - SW_mean\n",
    "    \n",
    "    # Perform L2 normalization (row-wise)\n",
    "    SW_norm = np.linalg.norm(SW_centered, axis=1, keepdims=True)\n",
    "    SW_normalized = SW_centered / SW_norm\n",
    "\n",
    "    return SW_normalized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999dca0-a575-4403-88ea-71be1c4b1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Compute Sliding Window Point Cloud\n",
    "\n",
    "SW1 = SW_cloud_nD(cs1,t_vals,t1,d,300,2)\n",
    "SW2 = SW_cloud_nD(cs2,t_vals,t2,d,300,2)\n",
    "\n",
    "pca = PCA(n_components=2) \n",
    "proj_2D_1 = pca.fit_transform(SW1)\n",
    "proj_2D_2 = pca.fit_transform(SW2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))  \n",
    "\n",
    "axes[0].scatter(proj_2D_1[:,0], proj_2D_1[:,1], s=10, alpha=0.7, color='deepskyblue')\n",
    "axes[0].set_title(fr'Raw Keypoint SW Point Cloud')\n",
    "\n",
    "axes[1].scatter(proj_2D_2[:,0], proj_2D_2[:,1], s=10, alpha=0.7, color='darkcyan')\n",
    "axes[1].set_title(fr'Acceleration SW Point Cloud')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c52fb-840c-46da-a26b-a1c2aeeb451c",
   "metadata": {},
   "source": [
    "### Peridoicity Score\n",
    "\n",
    "To measure the \"roundness\" of the high dimensional point cloud, we use persistent homology to derive a periodicity score:\n",
    "\n",
    "$$PS_1 = w_1\\frac{mp_1(dgm_1)}{\\sqrt3} - w_2\\frac{mp_2(dgm_1)}{\\sqrt3}$$\n",
    "\n",
    "This approach directly measures the offset between the first and second most persistent $H_1$ features. The bigger this offset is, the more circular we expect our $S\\mathbb{W}$ point cloud to be. We also offer the option to use an alternative periodicity score that is more of a summary of the 10 most persistent $H_1$ features\"\n",
    "\n",
    "$$PS_{10} = \\begin{bmatrix} w_1\\frac{mp_1(dgm_1)}{\\sqrt3}, w_2\\frac{mp_2(dgm_1)}{\\sqrt3}, \\dots ,  w_n\\frac{mp_n(dgm_1)}{\\sqrt3} \\end{bmatrix} $$\n",
    "\n",
    "While this provides a more discriptive measurement of the persistence diagram, the overall accuracy increase is negligible compared to $PS_1$ and is much less interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe03e1b-5ea4-4204-978b-1faac84696e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "\n",
    "def next_prime(n):\n",
    "    '''\n",
    "    A general rule of thumb is for the coeff parameter in Ripser to be set to the next prime after 2*d\n",
    "    '''\n",
    "    return sympy.nextprime(n)\n",
    "\n",
    "def compute_PS(dgm1, method = 'PS1'):\n",
    "    try:\n",
    "        #Sort the points to get the 10 most persistent points\n",
    "        sorted_points = sorted(dgm1, key=lambda x: x[1] - x[0], reverse=True)\n",
    "        n_most_persistent_points = np.array(sorted_points[:10])\n",
    "        zeros_array = np.zeros((10 - len(n_most_persistent_points), 2))\n",
    "        filled_points = np.vstack([n_most_persistent_points, zeros_array])\n",
    "        persistences = np.abs(filled_points[:, 0] - filled_points[:, 1]) \n",
    "        weights =  np.exp(-0.5 * filled_points[:, 0])\n",
    "    except:\n",
    "        persistences = np.zeros(10)\n",
    "        weights =  np.zeros(10)\n",
    "        \n",
    "    if method == 'PS10':\n",
    "        return (persistences * weights) / np.sqrt(3)\n",
    "    elif method == 'PS1':\n",
    "        return (persistences[0] * weights[0]) / np.sqrt(3) - (persistences[1] * weights[1]) / np.sqrt(3)\n",
    "    else:\n",
    "        print('Not a valid mathod. Choose either 10MPS or 1PS')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a97898-8fa6-4d75-ba95-d5cad12aeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Compute Homology from Rips Filtration\n",
    "'''\n",
    "\n",
    "#Rips filtration for homology for both point clouds\n",
    "result1 = ripser(SW1, coeff=next_prime(2*d), maxdim = 1) \n",
    "result2 = ripser(SW2, coeff=next_prime(2*d), maxdim = 1) \n",
    "\n",
    "#Extract diagrams\n",
    "diagrams1 = result1['dgms']\n",
    "diagrams2 = result2['dgms']\n",
    "\n",
    "dgm1_1 = diagrams1[1]\n",
    "dgm1_1 = np.array(dgm1_1)\n",
    "\n",
    "dgm1_2 = diagrams2[1]\n",
    "dgm1_2 = np.array(dgm1_2)\n",
    "\n",
    "method = 'PS1'\n",
    "scores1 = compute_PS(dgm1_1, method = method)\n",
    "scores2 = compute_PS(dgm1_2, method = method)\n",
    "\n",
    "try: \n",
    "    num = len(scores1)\n",
    "    x_lim = 9.5\n",
    "except:\n",
    "    num = 1\n",
    "    x_lim = 0.5\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10)) \n",
    "\n",
    "plot_diagrams(diagrams1, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[0,0])\n",
    "axes[0,0].set_title(fr'Raw Position Persistence Diagram')\n",
    "\n",
    "axes[0,1].bar(range(num), scores1, alpha=0.5)\n",
    "axes[0,1].set_title(fr'Raw Position PS')\n",
    "axes[0,1].set_xlim(-0.5, x_lim)\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "axes[0,1].set_xticks([])\n",
    "\n",
    "plot_diagrams(diagrams2, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[1,0])\n",
    "axes[1,0].set_title(fr'Acceleration Persistence Diagram')\n",
    "\n",
    "axes[1,1].bar(range(num), scores2, alpha=0.5)\n",
    "axes[1,1].set_title(fr'Acceleration PS')\n",
    "axes[1,1].set_xlim(-0.5, x_lim)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243ff9d-1464-42d3-a4c5-6965683fd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 7)) \n",
    "\n",
    "axes[0,0].plot(t_vals,keypoint_x,color='r',label = 'X')\n",
    "axes[0,0].plot(t_vals,keypoint_y,color='g',label = 'Y')\n",
    "\n",
    "axes[0,0].set_title(\"Chest Spatial Positions\")\n",
    "axes[0,0].set_xlabel(\"Time\")\n",
    "axes[0,0].set_ylabel(\"Normalized Position\")\n",
    "axes[0,0].set_yticks([])\n",
    "axes[0,0].set_xticks([])\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Second subplot\n",
    "axes[1,0].plot(t_vals,keypoint_x_accel,color='r',label = 'X')\n",
    "axes[1,0].plot(t_vals,keypoint_y_accel,color='g',label = 'Y')\n",
    "axes[1,0].set_title(\"Chest Acceleration\")\n",
    "axes[1,0].set_xlabel(\"Time\")\n",
    "axes[1,0].set_ylabel(\"Acceleration\")\n",
    "axes[1,0].set_yticks([])\n",
    "axes[1,0].set_xticks([])\n",
    "axes[1,0].legend()\n",
    "\n",
    "\n",
    "axes[0,1].plot(frequencies1,magnitude_spectrum1)\n",
    "try:\n",
    "    axes[0,1].plot(frequencies1[pk1],magnitude_spectrum1[pk1],'x')\n",
    "    axes[0,1].vlines(peak_center1, ymin=0, ymax=np.max(magnitude_spectrum1[pk1]), colors='r', linestyles='dashed',label = 'F-Freq.')\n",
    "except:\n",
    "    print('Raw frequency detection failed')\n",
    "axes[0,1].set_xlabel(fr'$\\omega$')\n",
    "axes[0,1].set_ylabel(fr'$|F(\\omega)|$')\n",
    "axes[0,1].set_xticks([])\n",
    "axes[0,1].set_yticks([])\n",
    "axes[0,1].set_title(\"Magnitude Spectrum\")\n",
    "axes[0,1].legend()\n",
    "\n",
    "axes[1,1].plot(frequencies2,magnitude_spectrum2)\n",
    "try:\n",
    "    axes[1,1].plot(frequencies2[pk2],magnitude_spectrum2[pk2],'x')\n",
    "    axes[1,1].vlines(peak_center2, ymin=0, ymax=np.max(magnitude_spectrum2[pk2]), colors='r', linestyles='dashed',label = 'F-Freq.')\n",
    "except:\n",
    "    print('Raw frequency detection failed')\n",
    "axes[1,1].set_xlabel(fr'$\\omega$')\n",
    "axes[1,1].set_ylabel(fr'$|F(\\omega)|$')\n",
    "axes[1,1].set_xticks([])\n",
    "axes[1,1].set_yticks([])\n",
    "axes[1,1].set_title(\"Magnitude Spectrum\")\n",
    "axes[1,1].legend()\n",
    "\n",
    "axes[0,2].scatter(proj_2D_1[:,0], proj_2D_1[:,1], s=10, alpha=0.7, color='deepskyblue')\n",
    "axes[0,2].set_xticks([])\n",
    "axes[0,2].set_yticks([])\n",
    "axes[0,2].set_title(fr'PCA SW Point Cloud')\n",
    "\n",
    "axes[1,2].scatter(proj_2D_2[:,0], proj_2D_2[:,1], s=10, alpha=0.7, color='darkcyan')\n",
    "axes[1,2].set_xticks([])\n",
    "axes[1,2].set_yticks([])\n",
    "axes[1,2].set_title(fr'PCA SW Point Cloud')\n",
    "\n",
    "\n",
    "plot_diagrams(diagrams1, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[0,3])\n",
    "axes[0,3].set_xticks([])\n",
    "axes[0,3].set_yticks([])\n",
    "\n",
    "axes[0,3].set_title(fr'Persistence Diagram')\n",
    "\n",
    "axes[0,4].bar(range(num), scores1, alpha=0.5)\n",
    "axes[0,4].set_title(fr'$PS_1$')\n",
    "axes[0,4].set_xlim(-0.5, x_lim)\n",
    "axes[0,4].set_ylim(0, 1)\n",
    "axes[0,4].set_xticks([])\n",
    "\n",
    "plot_diagrams(diagrams2, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[1,3])\n",
    "axes[1,3].set_xticks([])\n",
    "axes[1,3].set_yticks([])\n",
    "\n",
    "axes[1,3].set_title(fr'Persistence Diagram')\n",
    "\n",
    "axes[1,4].bar(range(num), scores2, alpha=0.5)\n",
    "axes[1,4].set_title(fr'$PS_1$')\n",
    "axes[1,4].set_xlim(-0.5, x_lim)\n",
    "axes[1,4].set_ylim(0, 1)\n",
    "axes[1,4].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
