{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0401395e-0813-4cc7-b9c3-1397e9df451f",
   "metadata": {},
   "source": [
    "# Tutorial: Data Analysis with Pre-Processed Motion Data  \n",
    "\n",
    "Austin MBaye\n",
    "\n",
    "## **Overview**\n",
    "In this tutorial, we will walk through the **core data analysis** process for our research using a `.pkl` file. This file contains de-identified motion data from all videos and studies, serving as the foundation for our analysis. \n",
    "\n",
    "###  **What’s in the `.pkl` File?**\n",
    "The `.pkl` file includes key motion tracking data extracted from MediaPipe’s **BlazePose model**, along with additional metadata:\n",
    "\n",
    "- **`keypoints`** – The **(x, y, visibility)** landmark (or also referred to as keypoint) coordinates for each joint detected in the BlazePose model.\n",
    "- **`annotations`** – Frame-wise annotations for each video, indicating labeled behaviors.\n",
    "- **`fps`** – Frames per second (fps) of the video.\n",
    "- **`frame_count`** – Total number of frames in each video.\n",
    "- **`duration`** – Video duration in seconds.\n",
    "- **`name`** – Unique identifier for each entry, structured as (child-date_study).\n",
    "\n",
    "This `.pkl` file contains all necessary pre-processing data for the analysis, while still allowing room for further modifications or new ideas using the raw MediaPipe keypoint data.\n",
    "\n",
    "---\n",
    "\n",
    "## **AQSM-SW1PerS + MediaPipe Algorithm Walkthrough**\n",
    "The **Automated Quantification of Stereotypical Motor Movements via Sliding Windows and 1-Persistence Scoring (AQSM-SW1PerS)** pipeline follows these key steps:\n",
    "\n",
    "1.  **Data Colletion** - Optional walkthrough that shows how videos were created and MediaPipe pose inference is run\n",
    "2.  **Preprocessing** – Cleaning and structuring motion data for analysis.  \n",
    "3.  **Period Estimation** – Identifying periodic patterns in movement.  \n",
    "4.  **Optimizing Parameters $d$ and $\\tau$** – Finding the best delay embedding parameters for persistence analysis.  \n",
    "5.  **Computing Sliding Window Embedding** – Transforming motion data into time-delayed embeddings.  \n",
    "6.  **Computing Persistent Homology & Extracting Features** – Calculating topological persistence and selecting the 10 most persistent points as motion descriptors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554ac9e-d544-4e5d-9ad6-6b0e96075a06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Collection (Optional Walkthrough)\n",
    "\n",
    "This part of the walkthrough shows how videos were created from publically available data from *Goodwin et. al.* and how to run MediaPipe inference on the videos. As we give all the critical information on the videos and provide all MediaPipe pose landmarks in our `.pkl` file, this section is not critical to any data analysis we do in other sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce130a0-7cf6-4a9a-83dc-0e0487e782ca",
   "metadata": {},
   "source": [
    "###  **Loading annotations**\n",
    "\n",
    "The function below extracts annotation information from Goodwin et. al 2014, and are essential for writing the video files. The following are located in **`AQSM_SW1PerS/utils/video_tools.py`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e239f-8758-45ed-b77e-738d49a33f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AQSM_SW1PerS.utils.video_tools import *\n",
    "from AQSM_SW1PerS.utils.paths import get_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d267e4-8053-4089-99e9-12cfc5032d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'Study1/URI-001-01-18-08/Annotator1Stereotypy.annotation.xml'  #Replace with the path of any study/session\n",
    "data_path = get_data_path(\"data\", data_folder)\n",
    "\n",
    "annotations, good_data = loadAnnotations(data_path)\n",
    "\n",
    "#Look at annotations\n",
    "for anno in annotations:\n",
    "    print(np.vstack(anno))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10157e-3609-467d-9ab4-1933cee720a9",
   "metadata": {},
   "source": [
    "###  **Writing Videos**\n",
    "\n",
    "We can write videos choosing frames that are within the UNIX start and end times (*good_data*) from the annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27c320-cf17-4f0c-be13-2d97e75ff631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames_path = 'Study1/001/URI-001-01-18-08/2008-01-18' #Replace with the path of any study/session\n",
    "\n",
    "folder_path = get_data_path(\"autismvideos\", frames_path)\n",
    "\n",
    "output_video = '001-01-18-08' \n",
    "\n",
    "frame_annotations = write_video(folder_path, output_video, good_data, annotations)\n",
    "\n",
    "video_path = f'{output_video}_part_0.avi'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a599b-e901-4c69-b3f5-0ab19542d4e2",
   "metadata": {},
   "source": [
    "###  **MediaPipe & YOLOv5 Pose Estimation** \n",
    "\n",
    "MediaPipe is an open-source framework developed by Google that provides efficient, cross-platform solutions for real-time machine learning pipelines. In this project, we leverage MediaPipe's pose landmarker to capture body movements at a low computational cost compared to other models, such as OpenPose. This task uses a machine learning model on a continuous stream of images. The task outputs 33 landmarks. The following class can be located in `**AQSM_SW1PerS/mediapipe_pose.py**`\n",
    "\n",
    "The class ``MP_pose`` maybe be initalized with:\n",
    " - `time_values` (ndarray (N, 1)): timestamps for each frame of video.\n",
    " - `fps` (float): frames per second of video.\n",
    " - `model` (path): YOLOv5 model.\n",
    " - `create_video` (bool): When set to True, video with skeleton overlayed on person of interest will be saved (Default = True).\n",
    " - `min_detection_confidence` (float): The minimum confidence score for the pose detection to be considered successful (Deafult = 0.3).\n",
    " - `min_tracking_confidence` (float): The minimum confidence score for the pose tracking to be considered successful (Deafult = 0.3).\n",
    " - `model_complexity` (int): Pose landmarker model used. 0 indicates the smallest and fastest model offered while 2 indicates the largest and most robust model offered (Default = 2).\n",
    "\n",
    "To extract the video information (e.g., time_values, and fps) ``*AQSM_SW1PerS.utils.data_processing**`` provides functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31b84e-5d04-4110-9f97-5d962f75f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AQSM_SW1PerS.mediapipe_pose import *\n",
    "from AQSM_SW1PerS.utils.data_processing import *\n",
    "\n",
    "windows_os = False #If you are using a windows OS, you may need to turn this to true since we trained YOLO model on UNIX\n",
    "if windows_os:\n",
    "    import pathlib\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "model_path = get_data_path(\"YOLOv5l/yolov5/runs/train/exp/weights\", \"YOLOv5l_transfer.pt\") \n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
    "\n",
    "fps, total_frames, duration_seconds, frame_times = get_video_info(video_path)\n",
    "\n",
    "MPPose = MP_pose(frame_times, fps, model, create_video = False)\n",
    "\n",
    "MPPose._run_pose_inference(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebc409-f809-4aa0-98ba-094240f0eef0",
   "metadata": {},
   "source": [
    "# This marks the end of the optional processing of the available data from Goodwin et. al. For the remainder of the tutorial, we use the `*.pkl*` file which contains all of the data we collected above into one easy-to-use dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcf36b-0473-4a55-a3cf-287e90d4c798",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The following functions are located in **`AQSM_SW1PerS/utils/data_processing.py`**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e019dc-63f8-4220-a3b4-7f1323ee6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AQSM_SW1PerS.utils.data_processing import *\n",
    "from AQSM_SW1PerS.utils.paths import get_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af5ecf-6aec-4d3a-96f4-60ce66b6168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = get_data_path(\"dataset.pkl\")\n",
    "\n",
    "data = open_pickle(pkl_file = pkl_file)\n",
    "\n",
    "index = 0\n",
    "for entry in data:  # First video entry as example\n",
    "   \n",
    "    print(f\"Video Name: {entry['name']}, Index: {index}\")\n",
    "    index += 1\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94e64b-395c-492c-9a0b-553780a3e8f8",
   "metadata": {},
   "source": [
    "### Step 1. Window Segmentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7203d27-d271-40de-83ce-30dba7ab9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = data[13] \n",
    "fps, frame_times, segments, annotated_segments = segment_video(entry, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9bec4-e9ed-4acf-bc13-73437a8a86fe",
   "metadata": {},
   "source": [
    "### Step 2. Landmark Extraction \\& Smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc01ee-e917-48d9-b0ec-4add688475df",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = extract_landmarks(entry, 0, frame_times, fps)\n",
    "lshoulder = extract_landmarks(entry, 11, frame_times, fps)\n",
    "rshoulder = extract_landmarks(entry, 12, frame_times, fps)\n",
    "rwrist = extract_landmarks(entry, 16, frame_times, fps, do_wrists=True, elbow_index=14, shoulder_index=12)\n",
    "lwrist = extract_landmarks(entry, 15, frame_times, fps, do_wrists=True, elbow_index=13, shoulder_index=11)\n",
    "\n",
    "chest = getChest(head, lshoulder, rshoulder)\n",
    "\n",
    "lshoulder_accel = getAccel(lshoulder,fps)\n",
    "rshoulder_accel = getAccel(rshoulder,fps)\n",
    "\n",
    "rwrist_accel = getAccel(rwrist,fps)\n",
    "lwrist_accel = getAccel(lwrist,fps)\n",
    "\n",
    "head_accel = getAccel(head,fps)\n",
    "chest_accel = getAccel(chest,fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aa081-af41-4fe9-94cf-8dfa811f9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Landmark spatial positions splines\n",
    "\n",
    "h_x, h_y = CubicSpline(frame_times,head[:,0]), CubicSpline(frame_times,head[:,1])\n",
    "\n",
    "r_x, r_y = CubicSpline(frame_times,rwrist[:,0]), CubicSpline(frame_times,rwrist[:,1])\n",
    "\n",
    "l_x, l_y = CubicSpline(frame_times,lwrist[:,0]), CubicSpline(frame_times,lwrist[:,1])\n",
    "\n",
    "rs_x, rs_y = CubicSpline(frame_times,rshoulder[:,0]), CubicSpline(frame_times,rshoulder[:,1])\n",
    "\n",
    "ls_x, ls_y = CubicSpline(frame_times,lshoulder[:,0]), CubicSpline(frame_times,lshoulder[:,1])\n",
    "\n",
    "c_x, c_y = CubicSpline(frame_times,chest[:,0]), CubicSpline(frame_times,chest[:,1])\n",
    "\n",
    "#Acceleration representation of landmark movement splines\n",
    "\n",
    "h_x_a, h_y_a = CubicSpline(frame_times,head_accel[:,0]), CubicSpline(frame_times,head_accel[:,1])\n",
    "\n",
    "r_x_a, r_y_a = CubicSpline(frame_times,rwrist_accel[:,0]), CubicSpline(frame_times,rwrist_accel[:,1])\n",
    "\n",
    "l_x_a, l_y_a = CubicSpline(frame_times,lwrist_accel[:,0]), CubicSpline(frame_times,lwrist_accel[:,1])\n",
    "\n",
    "rs_x_a, rs_y_a = CubicSpline(frame_times,rshoulder_accel[:,0]), CubicSpline(frame_times,rshoulder_accel[:,1])\n",
    "\n",
    "ls_x_a, ls_y_a = CubicSpline(frame_times,lshoulder_accel[:,0]), CubicSpline(frame_times,lshoulder_accel[:,1])\n",
    "\n",
    "c_x_a, c_y_a = CubicSpline(frame_times,chest_accel[:,0]), CubicSpline(frame_times,chest_accel[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541418a-9756-4503-a3fc-ab1cb90f9372",
   "metadata": {},
   "source": [
    "### Step 3. Analyze a time window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173ab9b-cfb7-47f8-be3d-e2b789e4b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: choose either a time value OR an annotation\n",
    "time_value = None     # set to a numeric value if you want to choose by time\n",
    "annotation = 1   # set to a string if you want to choose by annotation\n",
    "\n",
    "# Specify which occurrence you want (0 for first, 1 for second, etc.)\n",
    "occurrence = 1  # for example, choose the second occurrence\n",
    "\n",
    "if (time_value is None and annotation is None) or (time_value is not None and annotation is not None):\n",
    "    raise ValueError(\"Please provide either a time_value OR an annotation (but not both).\")\n",
    "\n",
    "if time_value is not None:\n",
    "    # Gather all matching segment indices based on time_value\n",
    "    matching_indices = [\n",
    "        i for i in range(len(segments) - 1)\n",
    "        if np.min(segments[i]) >= time_value and np.min(segments[i + 1]) != time_value\n",
    "    ]\n",
    "else:\n",
    "    # Gather all matching segment indices based on annotation\n",
    "    matching_indices = [i for i, ann in enumerate(annotated_segments) if ann == annotation]\n",
    "\n",
    "if not matching_indices:\n",
    "    raise ValueError(\"No segment found for the given criteria.\")\n",
    "\n",
    "if occurrence >= len(matching_indices):\n",
    "    raise ValueError(\"Occurrence number is out of range. There are only {} matches.\".format(len(matching_indices)))\n",
    "\n",
    "idx = matching_indices[occurrence]\n",
    "\n",
    "segment = segments[idx]\n",
    "annotated_segment = annotated_segments[idx]\n",
    "\n",
    "print(\"Chosen segment:\", segment)\n",
    "print(\"Chosen annotated segment:\", annotated_segment)\n",
    "\n",
    "\n",
    "# --- Choose a Sensor ---\n",
    "def choose_sensor(sensor='Chest'):\n",
    "    # Map sensor names to their corresponding (position, acceleration) tuples.\n",
    "    sensor_map = {\n",
    "        'Chest': ((c_x, c_y), (c_x_a, c_y_a)),\n",
    "        'LW':    ((l_x, l_y), (l_x_a, l_y_a)),\n",
    "        'RW':    ((r_x, r_y), (r_x_a, r_y_a)),\n",
    "        'LS':    ((ls_x, ls_y), (ls_x_a, ls_y_a)),\n",
    "        'RS':    ((rs_x, rs_y), (rs_x_a, rs_y_a)),\n",
    "        'Head':  ((h_x, h_y), (h_x_a, h_y_a))\n",
    "    }\n",
    "    \n",
    "    if sensor not in sensor_map:\n",
    "        raise ValueError(\"Not a valid sensor. Valid options are: \" + \", \".join(sensor_map.keys()))\n",
    "    \n",
    "    (t_x, t_y), (t_x_a, t_y_a) = sensor_map[sensor]\n",
    "    return t_x, t_y, t_x_a, t_y_a\n",
    "\n",
    "# Example usage:\n",
    "sensor = 'Chest'\n",
    "t_x, t_y, t_x_a, t_y_a = choose_sensor(sensor=sensor)\n",
    "\n",
    "spline_funcs_spatial = [t_x, t_y]\n",
    "spline_funcs_accel = [t_x_a, t_y_a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f363a08-9055-4005-b808-40269269dc2c",
   "metadata": {},
   "source": [
    "# SW1PerS (Sliding Windows & 1-Persistence Scoring)\n",
    "\n",
    "**File:** `AQSM_SW1PerS/SW1PerS.py`\n",
    "\n",
    "---\n",
    "\n",
    "**SW1PerS** quantifies motion periodicity by:\n",
    "1. Constructing sliding-window delay embeddings of the time series.  \n",
    "2. Applying persistent homology to capture 1-dimensional loops which are indicative of recurrent patterns in time series.  \n",
    "3. Scoring periodicity strength via the prominence of 1-persistence features.\n",
    "\n",
    "The output is a time-varying score that reflects how strongly cyclical the motion is.\n",
    "\n",
    "---\n",
    "\n",
    "The `SW1PerS` class encapsulates the full pipeline:\n",
    "\n",
    "- Preprocessing time series by detrending and increasing resolution\n",
    "- Period Estimation for optimal $d$ and $\\tau$ parameters.\n",
    "- Sliding Window Embedding Point Cloud generation\n",
    "- Persistent homology computation  \n",
    "- Periodicity scoring  \n",
    "\n",
    "---\n",
    "\n",
    "## Initialization\n",
    "\n",
    "- `start_time` (float): Start time of the time segment (Default = 0).\n",
    "- `end_time` (float): End time of the time segment (Default = 4).\n",
    "- `num_points` (int): Number of points to inteprolate between start and end times. >> number gives more accurate period estimation  (Default = 1000).\n",
    "- `method` (str): Periodicity scoring method $PS_1$ or $PS_{10}$ (Default = `PS1`).\n",
    "- `d` (int): Fixed parameter for sliding window embedding to ensure comapring point clouds in same ambient dimension (Default = 23).\n",
    "- `prime_coeff` (int): Parameter for `Ripser.py`. General rule of thumb is for `prime_coeff = next_prime(2*d)`.  \n",
    "- `f_min` (float): Minimum frequency value explored for period estimation (Default = 0.5 -> Max period = 2.0s).\n",
    "- `f_max` (float): Maximum frequency value explored for period estimation (Default = 2.0 -> Min period = 0.5s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2fbcf1-8519-43ed-9a47-72c426984129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AQSM_SW1PerS.SW1PerS import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6f7bc-8c4d-401e-bfce-ff62b1cf5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1000\n",
    "method = 'PS1'\n",
    "d = 23\n",
    "prime_coeff = next_prime(d)\n",
    "\n",
    "scoring_pipeline_spatial = SW1PerS(start_time = np.min(segment), end_time = np.max(segment), num_points = num_points, method = method, d = d, prime_coeff = prime_coeff)\n",
    "scoring_pipeline_accel = SW1PerS(start_time = np.min(segment), end_time = np.max(segment), num_points = num_points, method = method, d = d, prime_coeff = prime_coeff)\n",
    "\n",
    "scoring_pipeline_spatial._detrend_and_convert(spline_funcs_spatial)\n",
    "scoring_pipeline_accel._detrend_and_convert(spline_funcs_accel)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scoring_pipeline_spatial.time_values, scoring_pipeline_spatial.X_detrended[:,0], color='r', label='X')\n",
    "plt.plot(scoring_pipeline_spatial.time_values, scoring_pipeline_spatial.X_detrended[:,1], color='g', label='Y')\n",
    "plt.title(f\"{sensor} Landmarks (Spatial)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Normalized Position\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scoring_pipeline_accel.time_values, scoring_pipeline_accel.X_detrended[:,0], color='r', label='X')\n",
    "plt.plot(scoring_pipeline_accel.time_values, scoring_pipeline_accel.X_detrended[:,1], color='g', label='Y')\n",
    "plt.title(f\"{sensor} Landmarks (Accel)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb63131-53eb-412f-ba41-a18346067886",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_pipeline_spatial._estimate_period()\n",
    "period_pos = scoring_pipeline_spatial.period\n",
    "print(f'Estimated Period Spatial Position: {period_pos}')\n",
    "print('')\n",
    "scoring_pipeline_accel._estimate_period()\n",
    "period_accel = scoring_pipeline_accel.period\n",
    "print(f'Estimated Period Acceleration: {period_accel}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f093274-5d2f-4f27-9c30-96ab4e466537",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_pipeline_spatial._sliding_windows()\n",
    "scoring_pipeline_accel._sliding_windows()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "SW_spatial = scoring_pipeline_spatial.SW\n",
    "SW_accel = scoring_pipeline_accel.SW\n",
    "\n",
    "pca = PCA(n_components=2) \n",
    "proj_2D_spatial = pca.fit_transform(SW_spatial)\n",
    "proj_2D_accel = pca.fit_transform(SW_accel)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))  \n",
    "\n",
    "axes[0].scatter(proj_2D_spatial[:,0], proj_2D_spatial[:,1], s=10, alpha=0.7, color='deepskyblue')\n",
    "axes[0].set_title(fr'Spatial Landmark SW Point Cloud')\n",
    "\n",
    "axes[1].scatter(proj_2D_accel[:,0], proj_2D_accel[:,1], s=10, alpha=0.7, color='darkcyan')\n",
    "axes[1].set_title(fr'Acceleration Landmark SW Point Cloud')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7076038-7aaa-4ac5-9283-a0bedd964d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_pipeline_spatial._1PerS()\n",
    "scoring_pipeline_accel._1PerS()\n",
    "\n",
    "if method == 'PS10':\n",
    "    num = 10\n",
    "    x_lim = 9.5\n",
    "else:\n",
    "    num = 1\n",
    "    x_lim = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10)) \n",
    "\n",
    "plot_diagrams(scoring_pipeline_spatial.diagram, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[0,0])\n",
    "axes[0,0].set_title(fr'Spatial Position Persistence Diagram')\n",
    "\n",
    "axes[0,1].bar(range(num), scoring_pipeline_spatial.periodicity_score, alpha=0.5)\n",
    "axes[0,1].set_title(fr'Periodicity Score')\n",
    "axes[0,1].set_xlim(-0.5, x_lim)\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "axes[0,1].set_xticks([])\n",
    "\n",
    "plot_diagrams(scoring_pipeline_accel.diagram, plot_only=[1], xy_range=[0, 2, 0, 2], ax = axes[1,0])\n",
    "axes[1,0].set_title(fr'Acceleration Persistence Diagram')\n",
    "\n",
    "axes[1,1].bar(range(num), scoring_pipeline_accel.periodicity_score, alpha=0.5)\n",
    "axes[1,1].set_title(fr'Periodicity Score')\n",
    "axes[1,1].set_xlim(-0.5, x_lim)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
